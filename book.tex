%% -*- Mode: LaTeX; Coding: utf-8; -*-
%% Invitation to Algorithms with Scheme

\documentclass[12pt,openright,draft]{book}

%% ----- PACKAGES START ----- %%

%% TODO(rml): Figure out what packages you want to use.  We should
%% ensure that whatever we use also works with tex2page.

%% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
%% \usepackage{listings}

%% https://www.overleaf.com/learn/latex/Typesetting_quotations
%% For putting quotations at the beginning of each chapter.
%% \usepackage{epigraph}

%% http://kb.mit.edu/confluence/x/8Z07
%% For making the margins smaller.
\usepackage[left=0.618in,right=0.618in,top=1in,bottom=1in]{geometry}

%% For hyperlinks.
%% TODO(rml): Add link targets to the document.
\usepackage{hyperref}

%% ----- PACKAGES END ----- %%

%% Custom page layout

%% https://en.wikibooks.org/wiki/LaTeX/Paragraph_Formatting

%% TODO(rml): Figure out paragraph spacing and indentation.

%% Don't indent new paragraphs as much.
%% \setlength{\parindent}{0.2cm} % Default is 15pt.

%% Add extra vertical space before each paragraph.
%% \setlength{\parskip}{0.5cm plus4mm minus3mm}

%% End custom page layout

\begin{document}
\title{Invitation to Algorithms with Scheme}
\author{R. M. Loveland}
\date{November 2018}

\frontmatter{}
\maketitle{}

\tableofcontents{}

\chapter{Preface}

Why another book about algorithms?

There are already many books about algorithms.  Most provide detailed
analyses of topics such as ``Big O notation'' and are aimed at an
academic audience.  As a result, they strive to be comprehensive in
their treatment of the topic, and may run to many hundreds of pages in
length.

Most books use ``industry-standard'' programming languages like Java,
C++, or even Python.  Some use pseudocode, which is arguably better,
or at least not as ephemeral (the world will not always program
computers in Java).  It's all very well, but we are interested in
implementing algorithms in Scheme, which is not an ALGOL-derived
language which distinguishes between statements and expressions and
``fundamental'' data types and ``objects''?

Why Scheme?  Because Java, C++, and Python do not have anything
FUNDAMENTAL to say about computing.  They are languages of the moment.
By contrast, Scheme is a small, well-designed language that is not
tied to particular programming paradigms or hardware architectures.
There have even been Scheme CPUs.  It is based on fundamental ideas
about an ideal language for computing, that is, expressing the idea of
a computational process.  In their paper ``Lisp: A language for
stratified design'', Abelson and Sussman write:

Programming languages should be designed not by piling feature on top
of feature, but by removing the weaknesses and restrictions that make
additional features appear necessary.  The Scheme dialect of Lisp
demonstrates that a very small number of rules for forming
expressions, with no restrictions on how they are composed, suffice to
form a practical and efficient programming language that is flexible
enough to support most of the major programming paradigms in use
today.

When we say Scheme is not tied to any particular programming paradigm,
we mean that it can be extended by the programmer to use any paradigm:
imperative; functional; object-oriented; declarative; and more.

There is something different about S-expressions, the fundamental data
structure of Scheme programs, than about the data structures that are
used to represent other programming languages.  For one thing,
S-expressions possess a conceptual unity, since every expression takes
the form of symbols inside nested parentheses, e.g., this procedure to
compute the greatest common denominator of two numbers:

\begin{verbatim}
(define (gcd a b)
  (if (= b 0)
      a
    (gcd b (remainder a b))))
\end{verbatim}

As a result of this unity, Lisp programs can express metalinguistic
abstractions.  In other words, unlike in say, Java, Python, or Go, the
programmer is able to add locally-defined syntax to the language that
will allow her to better express her intent when describing a
computational process.

Finally, there is very little written material out there for the
novice-to-intermediate Scheme programmer (though there are many
excellent programs, some of which we will link to in the references).
Most of the Scheme content that is available on the internet is either
for experts, or misleads beginners with naive, tree-recursive
solutions that perform badly (I am certainly guilty of this on my own
blog).

Naturally, there are a number of excellent books on Scheme
programming, which we will link to from the bibliography.  However, to
our knowledge there are no algorithms books that use Scheme.  That is
the niche this book is attempting to fill.

\chapter{Introduction}

What is an \emph{algorithm}?

An algorithm is a recipe for getting a job done.  Many algorithms are
very simple, since many of the jobs we need a computer to do are (or
appear to be) quite simple.  For example, we may need to sort a list
of numbers, or calculate the distance between two towns on a map.

``Sort a list of numbers'' sounds simple, but there is some complexity
there that needs to be addressed.  How long is the list?  Is it longer
than the available memory on our machine?  Are the numbers in the list
in totally random order?  Or are they almost sorted already?

Depending on the answers to these questions, you'll need to choose one
sorting algorithm over another.  Some take more memory (space).  Some
require more CPU cycles (time).

Unless you are doing very specialized optimization work, you can
probably get along quite nicely knowing one or two of the most common
algorithms in the area you're currently working in.  If you really
need something more advanced, you can always look it up and implement
it.

And that brings us to the point of this book.  We will study one or
two common algorithms of each type (sorting, searching, etc.).  Then,
we will implement these algorithms in the Scheme programming language.
As our mastery increases in later chapters, we will take on several
``real world'' implementation projects, such as writing our own hash
tables and a regular expression matcher, all using the basic
algorithms we've learned.

Along the way, we will observe several (related) themes that repeat:

\begin{enumerate}
\item \emph{Divide and conquer}: break a problem into smaller
  sub-problems, solve the sub-problems, and combine the intermediate
  results to get a final answer.
\item \emph{Recursion}: Do it again, and again, and again, until
  you're done.
\end{enumerate}

\chapter{Prerequisites}

\section{Familiarity with Scheme}

This book does not try to teach the Scheme language.  We assume you
have already encountered an introduction to Scheme elsewhere. In
particular, you should be pretty comfortable with recursion, since
we'll be using it a lot (Scheme uses recursion for iteration by
default).  By ``comfortable with recursion'', we mean that you
understand code that looks like this:

\begin{verbatim}
(define (+ a b)
  (if (= a 0)
      b
    (+ (decr a)
       (incr b))))
\end{verbatim}

A good book for learning the basics of Scheme is \emph{The Little
  Schemer} by Dan Friedman \& Matthias Felleisen.  For more
reading recommendations, see the Bibliography. % TODO(rml): link

\section{Access to a computer with a Scheme Interpreter}

This book's code has been tested with the following combinations of
Scheme implementation and operating system:

%% TODO(rml): Fill in tested version numbers.
\begin{tabular}{ l m r }
Larceny        & X     & Windows 10, Linux \\
Chez (Petite)  & X     & Windows 10        \\
Gambit         & X     & Windows 10, Linux \\
Scheme 48      & 1.9.2 & Windows 10, Linux \\
Kawa           & 3.0   & Windows 10, Linux \\
JScheme        & 7.2   & Windows 10
\end{tabular}

\section{Required libraries}

The portable module system (which has been tested in all of the
implementation/OS combinations in the previous section) can be
downloaded from \url{https://github.com/rmloveland/load-module}.

The code for working with this book can be found at
\url{https://github.com/rmloveland/intro-algos-with-scheme/tree/master/code}.

To load the book's code:

%% TODO(rml): Update these instructions.
\begin{enumerate}

\item Copy \verb|load-module/load-module.scm| into the book's
  \verb|intro-algos-with-scheme/code| directory.

\item Change into the \verb|intro-algos-with-scheme/code| directory.

\item Start your Scheme.

\item Load the module system as follows:
  \verb|(load "load-module.scm")|

\item Load the required modules as described at the beginning of each
  chapter.
  E.g., \verb|(load-module 'mergesort)|.

\end{enumerate}

\section{Typographical Conventions}

Procedure names are written in capital letters like this:
\verb|CHAR-READY?|

Code samples are usually set off from the surrounding text like this:

\begin{verbatim}
(define (double n)
  (+ n n))
\end{verbatim}

Most procedures are annotated with comments about the input and output
types we expect.  These comments serve as a form of documentation.
They makes it a little easier to remember (at least part of) what a
procedure does without having to read the entire text of the
procedure.

For example, given the following procedure we can see that it takes a
two Numbers and returns a Number.

\begin{verbatim}
  (define (plus a b)
    ;; Num Num -> Num
    (+ a b))
\end{verbatim}

Procedures that are meant to be ``internal'' A.K.A. not part of a
user-visible API are prefixed with a \verb|^| character, e.g.,

\begin{verbatim}
  (define (^merge pred a b)
    ;; Pred List List -> List
    ;; Note: this implementation is a no-op.
    '())
\end{verbatim}

%% TODO(rml): Finish filling this in -- look at an ORA book for ideas.

\mainmatter{}
\part{Basics}
\chapter{Sorting, Searching, and Shuffling}

The list data structure is ubiquitous in Scheme programming.  One of
the most common patterns is to gather up a list of elements, and then
process them in turn.  (When we say that something is a ``list'' in
Scheme, we mean that it's actually a linked list.)

For example, we might like to walk down through a directory of files,
checking each file for some interesting property.

\begin{verbatim}
(define (dir-walk* interesting? queue)
  (let ((file #f))
    (lambda ()
      (if (not (null? queue))
          (begin (set! file (car queue))
                 (set! queue (cdr queue))
                 (cond
                  ((file-directory? file)
                   (let ((new-files (directory-files file)))
                     (set! queue
                           (append queue
                                   (filter interesting? (map (lambda (filename)
                                                               (string-append file "/" filename))
                                                             new-files))))
                     (if (interesting? file) file)))
                  ((interesting? file) file)
                  (else #f)))
        #f))))
\end{verbatim}

XXX: Intro needs to be completely (re)written.

We'll begin our discussion of sorting by implementing the merge sort
algorithm.  This is useful for a few reasons:

\begin{enumerate}
\item It's a pretty good sorting algorithm.
\item It's a good example of the generally useful ``divide and
  conquer'' strategy for algorithm design.
\item It suits Scheme because of the way Scheme lists are
  actually ``linked lists''.
\end{enumerate}

It's also useful because we need to be able to sort before we can
search.  As a general rule in this book, we will not use a technique
until we have implemented the prerequisite algorithms.  As we just
said, a list has to be sorted before it can be searched (efficiently).

If you can understand the merge sort implementation described in this
chapter, you should have no problem learning more about (and
implementing your own versions of) other sorting algorithms that you
encounter.

We'll look at several implementations of merge sort, starting with the
naive version you can easily find on the internet that generates a
recursive process, and which performs quite badly.  We will then
improve on that by implementing an iterative\footnote{An iterative
  process is one that does not consume growing amounts of stack space
  while it runs.} implementation that is necessarily a bit more
complex (but not terribly so), and which performs better.

XXX: Write the naive version and some tests that exercise it.  We can
run those same tests later on against the better implementation.

Merge sort is most easily implemented in two parts that work together:

\begin{enumerate}

\item An internal merging procedure that merges two partially sorted
  lists.  This procedure handles splicing two sublists together in an
  ordering determined by a predicate such as \verb|<|. It walks both
  lists, putting things in pairwise order based on which of them is
  (in the case of \verb|<|) less than the other.  However, it doesn't
  fully sort the lists; it only works on one item from each list at a
  time.  That's why we need the driver procedure (described next).

\item A user-accessible driver procedure that calls the internal
  merging procedure repeatedly until all of the sub-lists are sorted,
  at which point the entire list is sorted.  This feels a bit magical,
  but in fact there is no magic to it at all.

\end{enumerate}

Sometimes it's easier to understand something by looking at its inputs
and outputs.  Here are a few examples showing expected inputs and
outputs of \verb|MERGE|, which is our name for the internal merging
procedure.

Note: to use the code below

\begin{verbatim}
  > (load-module 'merge-sort)
  > (merge < '(2 191) '(18 45))
  (2 18 45 191)

  > (merge string<? '("scheme" "hacking") '("is" "fun"))
  ("is" "fun" "scheme" "hacking")
\end{verbatim}

You will now have the following additional procedures in your
environment:

\begin{itemize}
\item MERGE
\item MERGE-SORT
\item MERGE-SORT-TRACED works just as MERGE-SORT, but prints some
  some additional output to make it easier to visualize what is
  happening.  Specifically, it prints the intermediate results of
  recursive calls to MERGE to make it easier to see how MERGE-SORT
  works by building up bigger and bigger sorted sub-lists and merging
  them together.
\end{itemize}

The user-facing driver procedure will handle calling \verb|MERGE|
repeatedly and splicing the partially sorted lists returned by
successive calls to \verb|MERGE| together into a final list that is
fully sorted.

Here are examples showing some expected inputs and outputs of
MERGE-SORT:

\begin{verbatim}
  > (merge-sort '(17 51 55 13 12 75 98 48 98 89 68 86 89 51) <)
  (12 13 17 48 51 51 55 68 75 86 89 89 98 98)
\end{verbatim}

As we said above, \verb|MERGE| splices two lists together in a
pairwise ordering determined by a predicate.  In other words, it walks
two lists, let's call them $A$ and $B$, and compares the elements of
each list in turn.  It keeps another list, $C$, where it stores the
output.  If the current element from $A$ is less than the current
element from $B$, \verb|MERGE| pushes it onto the output list $C$.
Otherwise, it pushes the current element from $B$ onto the output
list.

Once \verb|MERGE| has traversed both of its input lists $A$ and $B$,
and processed all the elements of each, it returns its result.

For example, in order for
\verb|(let ((a '(12 75)) (b '(1024 55))) (merge a b <))| to return the
output \verb|(12 75 1024 55)|, \verb|MERGE| follows these steps:

\begin{enumerate}

\item Grab the first item of \verb|a| (12) and the first item of
  \verb|b| (1024).

\item Compare 12 and 1024 and pairwise order them using the predicate
  \verb|<|, yielding an intermediate list \verb|'(12 1024)|.

\item Grab the next element of \verb|a| (75) and \verb|b| (55).

\item Compare 75 and 55 using \verb|<|, and push the now-ordered pair
  onto another intermediate list to make \verb|'(55 75)|.  We now have
  two intermediate lists, (55 75) and (12 1024).

\item MERGE recurs on the intermediate lists '(12 1024) and '(55 75)
  Since these intermediate lists are each themselves sorted from the
  previous pass, it (again) walks them in pairwise order, grabbing 12
  and 55 and sorting them to make '(12 55), and grabbing 1024 and 75
  and sorting them to make '(75 1024).  It then puts the two together
  to make (12 55 75 1024).
  
\end{enumerate}

For another way of looking at how the above process works, use the
MERGE-SORT-TRACED procedure from the 'mergesort module.

\begin{verbatim}
> (merge-sort-traced '(12 1024 75 55) <)

12
1024

75
55

(55 75)
(12 1024)

(12 55 75 1024)
\end{verbatim}

As noted above, the output of \verb|MERGE| is not a sorted list.  It
has been put into ``pairwise order''.  This is a fancy way of saying
that we only compared two elements at a time, one each from $A$ and
$B$, as we were building it.

Based on our description of the merge algorithm above, let's try to
generate some inputs to MERGE and guess their expected outputs.

\break

Let's look at the ouputs of a few more calls to MERGE with 1, 2, 3, and
4-element lists, respectively, to build our intuition for its
behavior.

\begin{verbatim}
(merge '(769) '(485) <)
; => (485 769)

(merge '(769 1023) '(485 99) <)
; => (485 99 769 1023)

(merge '(769 1023 3) '(485 99 293) <)
; => (485 99 293 769 1023 3)

(merge '(769 1023 3 12) '(485 99 293 13) <)
; => (485 99 293 13 769 1023 3 12)
\end{verbatim}

We can see that:

\begin{enumerate}
  \item Given 1-element lists, it sorts the 2 elements using the
    predicate and returns a sorted list

  \item Given 2-element lists $A$ and $B$, it returns '(B1 B2 A1 A2).

  \item Given 3-element lists, it returns '(B1 b2 b3 a1 a2 a3).

  \item Given 4-element lists, it returns
\end{enumerate}

Are you seeing the pattern?  Oddly, it appears that MERGE is only
merging lists according to a check on the first elements of each list
using the supplied predicate, and then leaving the rest of the lists
in their original order.

Clearly this is useful but not sufficient to get a list sorted!
Intuitively we can see that MERGE is useful as it gets down to shorter
lists, since it is able to put everything in sorted order in the
``1-element lists'' case.

Next let's look at how MERGE behaves with some deliberately ``odd''
inputs.

\begin{verbatim}
  ; Case 1
  (merge '() '() <)
  ; => ()

  ; Case 2
  (merge 19 23 <)
  ; => (19 23)

  ; Case 3
  (merge 19 '(1 2 3) <)
  ; => (1 2 3 19)

  ; Case 4
  (merge '() '(485) <)
  ; => (485)

  ; Case 5
  (merge '19 () <)
  ; => (19)

  ; Case 6
  (merge '(12) '(485 99 293 13) <)
  ; => (12 485 99 293 13)

  ; Case 7
  (merge '(12023) '(485 99 293 13) <)
  ; => (485 99 293 13 12023)
\end{verbatim}

We can codify the above behavior in the following specification of
cases:

\begin{enumerate}
\item The ``base case'' of \verb|(merge '() '() <)| should return ().
  (Case 1)

\item MERGE needs to work on numbers as well as lists. (Case 2, 3)

\item MERGE should accept an empty list as one of its inputs.

\item MERGE should accept as input 2 lists which do not have the same
  number of elements.
\end{enumerate}



We can see from the test cases we generated above that we will have to
handle a number of cases to handle, namely:

\begin{enumerate}

\item MERGE takes two lists.  If the lists contain strings or numbers
  as elements, we will need to XXX

\item If LEFT and RIGHT are both numbers, ``listify'' them so
  MERGE-AUX can work with them.

\item If LEFT is just a number, ``listify'' it so MERGE-AUX can work with
  it.

\item Likewise, if RIGHT is just a number, ``listify'' it for
  MERGE-AUX.

\item If LEFT and RIGHT are empty, we're done merging. Return the
  result.

\item If LEFT and RIGHT still have elements to be processed, call PRED
  and run them through MERGE-AUX again.

\item If the cases above haven't matched, and LEFT is not NULL?, call
  MERGE-AUX again.

\item If the cases above haven't matched, and RIGHT is not NULL?, call
  MERGE-AUX again.

\end{enumerate}

\begin{verbatim}
  (define (^merge pred l r)
    ;; Procedure List List -> List
    (define (merge-aux pred left right result)
      (cond
       ((and (number? left)     ; Case 1.
             (number? right))
        (merge-aux pred (list left) (list right) result))
       ((number? left)          ; Case 2.
        (merge-aux pred (list left) right result))
       ((number? right)         ; Case 3.
        (merge-aux pred left (list right) result))
       ((and (null? left)       ; Case 4.
             (null? right))
        (reverse result))
       ((and (not (null? left)) ; Case 5.
             (not (null? right)))
        (if (pred (car left)
                  (car right))
            (merge-aux pred
                       (cdr left)
                       right
                       (cons (car left) result))
          (merge-aux pred
                     left
                     (cdr right)
                     (cons (car right) result))))
       ((not (null? left))      ; Case 6.
        (merge-aux pred (cdr left) right (cons (car left) result)))
       ((not (null? right))     ; Case 7.
        (merge-aux pred left (cdr right) (cons (car right) result)))
       (else #f)))              ; We should never get here.
    (merge-aux pred l r '()))
\end{verbatim}

\subsubsection{Merge Sort}

Recently I've begun a project to implement a number of basic
algorithms in Scheme, which I'd like to eventually grow into a free
(as in freedom) ebook. Having just done a Binary Search in Scheme, I
thought it would be fun to give merge sort a try.

According to the mighty interwebs, merge sort is a good choice for
sorting linked lists (a.k.a., Lisp lists). Unfortunately the only Lisp
merge sort implementation examples I've been able to find on the web
have been recursive, not iterative.

The implementation described here is an iterative, bottom-up merge
sort, written in a functional style. (I daren't say the functional
style, lest any real Scheme wizards show up and burn me to a crisp.)

First, generate a list of random numbers

In order to have something to sort, we need a procedure that generates
a list of random numbers --- note that the docstring is allowed by
MIT/GNU Scheme; YMMV with other Schemes.

\begin{verbatim}
(define (make-list-of-random-numbers list-length max)
  ;; Int Int -> List
  "Make a list of random integers less than MAX that's LIST-LENGTH long."
  (letrec ((maker
            (lambda (list-length max result)
              (let loop ((n list-length) (result '()))
                (if (= n 0)
                    result
                    (loop (- n 1) (cons (random max) result)))))))
    (maker list-length max '())))
\end{verbatim}

\subsubsection{Then, write a merge procedure}

This implementation of the merge procedure is a straight port of the
one described on the Wikipedia Merge Sort page, with one minor
difference to make the sort faster 1.

An English description of the merge operation is as follows:

If both items passed in are numbers (or strings), wrap them up in
lists and recur. (In this example we only care about sorting numbers)

If both lists are empty, return the result.

If neither list is empty:

If the first item in the first list is ``less than'' the first item in
the second list, cons it onto the result and recur.

Otherwise, cons the first item in the second list on the result and
recur.

If the first list still has items in it, cons the first item onto the
result and recur.

If the second list still has items in it, cons the first item onto the
result and recur.

If none of the above conditions are true, return \verb|\#f|. I put
this here for debugging purposes while writing this code; now that the
procedure is debugged, it is never reached. (Note: ``debugged'' just
means I haven't found another bug yet.)

\begin{verbatim}
(define (rml/merge pred l r)
  (letrec ((merge-aux
            (lambda (pred left right result)
              (cond
               ((and (number? left)
                     (number? right))
                (merge-aux pred
                           (list left)
                           (list right)
                           result))
               ((and (string? left)
                     (string? right))
                (merge-aux pred
                           (list left)
                           (list right)
                           result))
               ((and (null? left)
                     (null? right))
                (reverse result))
               ((and (not (null? left))
                     (not (null? right)))
                (if (pred (car left)
                          (car right))
                    (merge-aux pred
                               (cdr left)
                               right
                               (cons (car left) result))
                  (merge-aux pred
                             left
                             (cdr right)
                             (cons (car right) result))))
               ((not (null? left))
                (merge-aux pred (cdr left) right (cons (car left) result)))
               ((not (null? right))
                (merge-aux pred left (cdr right) (cons (car right) result)))
               (else #f)))))
    (merge-aux pred l r '())))
\end{verbatim}

We can run a few merges to get a feel for how it works. The comparison
predicate we pass as the first argument will let us sort all kinds of
things, but for the purposes of this example we'll stick to numbers:

\begin{verbatim}
> (rml/merge < '(360 388 577) '(10 811 875 995))
(10 360 388 577 811 875 995)

> (rml/merge < '(8 173 227 463 528 817) '(10 360 388 577 811 875 995))
(8 10 173 227 360 388 463 528 577 811 817 875 995)

> (rml/merge <
           '(218 348 486 520 639 662 764 766 886 957 961 964)
           '(8 10 173 227 360 388 463 528 577 811 817 875 995))
(8 10 173 218 227 348 360 388 463 486 520 528 577 639 662 764 766 811 817 875 886 957 961 964 995)
\end{verbatim}

\subsubsection{Finally, do a bottom up iterative merge sort}

It took me a while to figure out how to do the iterative merge sort in
a Schemely fashion. As usual, it wasn't until I took the time to model
the procedure on paper that I got somewhere. Here's what I wrote in my
notebook:

\begin{verbatim}
;;  XS                   |      RESULT
;;---------------------------------------------

'(5 1 2 9 7 8 4 3 6)            '()
    '(2 9 7 8 4 3 6)            '((1 5))
        '(7 8 4 3 6)            '((2 9) (1 5))
            '(4 3 6)            '((7 8) (2 9) (1 5))
                '(6)            '((3 4) (7 8) (2 9) (1 5))
                 '()            '((6) (3 4) (7 8) (2 9) (1 5))

;; XS is null, and RESULT is not of length 1 (meaning it isn't sorted
;; yet), so we recur, swapping the two:

'((6) (3 4) (7 8) (2 9) (1 5))  '()
          '((7 8) (2 9) (1 5))  '((3 4 6))
                      '((1 5))  '((2 7 8 9) (3 4 6))
                           '()  '((1 5) (2 7 8 9) (3 4 6))

;; Once more XS is null, but RESULT is still not sorted, so we swap
;; and recur again

'((1 5) (2 7 8 9) (3 4 6))      '()
                  '(3 4 6)      '((1 2 5 7 8 9))
                       '()      '((3 4 6) (1 2 5 7 8 9))

;; Same story: swap and recur!

'((3 4 6) (1 2 5 7 8 9))        '()
                     '()        '((1 2 3 4 5 6 7 8 9))

;; Finally, we reach our base case: XS is null, and RESULT is of
;; length 1, meaning that it contains a sorted list

'(1 2 3 4 5 6 7 8 9)
\end{verbatim}

This was a really fun little problem to think about and visualize. It
just so happens that it fell out in a functional style; usually I
don't mind doing a bit of state-bashing, especially if it's
procedure-local. Here's the code that does the sort shown above:

\begin{verbatim}
  (define (rml/merge-sort xs pred)
    (let loop ((xs xs)
               (result '()))
         (cond ((and (null? xs)
                     (null? (cdr result)))
                (car result))
               ((null? xs)
                (loop result
                      xs))
               ((null? (cdr xs))
                (loop (cdr xs)
                      (cons (car xs) result)))
               (else
                (loop (cddr xs)
                      (cons (rml/merge <
                                       (first xs)
                                       (second xs))
                            result))))))
\end{verbatim}

That's nice, but how does it perform?

A good test of our merge sort is to compare it to the system's
built-in sort procedure. In the case of MIT/GNU Scheme, we'll need to
compile our code if we hope to get anywhere close to the system's
speed. If your Scheme is interpreted, you don't have to bother of
course.

To make the test realistic, we'll create three lists of random
numbers: one with 20,000 items, another with 200,000, and finally a
giant list of 2,000,000 random numbers. This should give us a good
idea of our sort's performance. Here's the output of timing first two
sorts, 20,000 and 200,000 2:

\begin{verbatim}
;;; Load compiled code

(load "mergesort")
;Loading "mergesort.so"... done
;Value: rml/insertion-sort2

;;; Define our lists

(define unsorted-20000 (make-list-of-random-numbers 20000 200000))
;Value: unsorted-20000

(define unsorted-200000 (make-list-of-random-numbers 200000 2000000))
;Value: unsorted-200000

;;; Sort the list with 20,000 items

(with-timing-output (rml/merge-sort unsorted-20000 <))
;Run time:      .03
;GC time:       0.
;Actual time:   .03

(with-timing-output (sort unsorted-20000 <))
;Run time:      .02
;GC time:       0.
;Actual time:   .021

;;; Sort the list with 200,000 items

(with-timing-output (rml/merge-sort unsorted-200000 <))
;Run time:      .23
;GC time:       0.
;Actual time:   .252

(with-timing-output (sort unsorted-200000 <))
;Run time:      .3
;GC time:       0.
;Actual time:   .3
\end{verbatim}

As you can see, our sort procedure is on par with the system's for
these inputs. Now let's turn up the heat. How about a list with
2,000,000 random numbers?

\begin{verbatim}
;;; Sort the list with 2,000,000 items

(define unsorted-2000000 (make-list-of-random-numbers 2000000 20000000))
;Value: unsorted-2000000

(with-timing-output (rml/merge-sort4 unsorted-2000000 <))
;Aborting!: out of memory
;GC #34: took:   0.80 (100%) CPU time,   0.10 (100%) real time; free: 11271137
;GC #35: took:   0.70 (100%) CPU time,   0.90  (81%) real time; free: 11271917
;GC #36: took:   0.60 (100%) CPU time,   0.90  (99%) real time; free: 11271917

(with-timing-output (sort unsorted-2000000 <))
;Run time:      2.48
;GC time:       0.
;Actual time:   2.474

\end{verbatim}

No go. On a MacBook with 4GB of RAM, our merge sort runs out of
memory, while the system sort procedure works just fine. It seems the
wizards who implemented this Scheme system knew what they were doing
after all!

It should be pretty clear at this point why we're running out of
memory. In MIT/GNU Scheme, the system sort procedure uses vectors and
mutation (and is no doubt highly tuned for the compiler), whereas we
take a relatively brain-dead approach that uses lists and lots of
cons-ing. I leave it as an exercise for the reader (or perhaps my
future self) to rewrite this code so that it doesn't run out of
memory.

Footnotes:

1 An earlier implementation started off the sort by
``exploding'' the list to be sorted so that
\verb|'(1 2 3)| became \verb|'((1) (2) (3))|. This is convenient for
testing purposes, but very expensive. It's also unnecessary after the
first round of merging. We avoid the need to explode the list
altogether by teaching merge to accept numbers and listify them when
they appear. We could also do the same for strings and other types as
necessary.

2 For the definition of the with-timing-output macro, see here.

\subsection{Quick Sort}

\subsection{Tree Sort}

\subsection{Insertion Sort}

\section{Searching}

Now that we've sorted a list of elements, we can search it.  It
turns out that searching through a list of things is much faster if
you can sort that list first.

In this section we'll look at a particular type of search algorithm
called binary search.  Binary search is so named because it cuts
the search space in half with every iteration.

Unlike some other searches, binary search only works on ordered
lists of things.  That is why we had to go through the trouble of
sorting our list earlier: so that we could search through it now.

Load the binary search library:

\begin{verbatim}
(load-module 'binary-search)
\end{verbatim}

\subsection{Binary Search}

Binary search is a method for finding a specific item in a sorted
list. Here's how it works:

Take a guess that the item you want is in the middle of the current
search ``window'' (when you start, the search window is the entire
list).

If the item is where you guessed it would be, return the index (the
location of your guess).

If your guess is ``less than'' the item you want (based on a
comparison function you choose), recur, this time raising the
``bottom'' of the search window to the midway point.

If your guess is ``greater than'' the item you want (based on your
comparison function), recur, this time lowering the ``top'' of the
search window to the midway point.

In other words, you cut the size of the search window in half every
time through the loop. This gives you a worst-case running time of
about (/ (log n) (log 2)) steps. This means you can find an item in a
sorted list of 20,000,000,000 (twenty billion) items in about 34
steps.

\subsubsection{Reading lines from a file}

Before I could start writing a binary search, I needed a sorted list
of items. I decided to work with a sorted list of words from
/usr/share/dict/words, so I wrote a couple of little procedures to
make a list of words from a subset of that file. (I didn't want to
read the entire large file into a list in memory.)

Note: Both \verb|format| and the Lisp-inspired \verb|\#!optional|
keyword are available in MIT Scheme; they made writing the re-matches?
procedure more convenient.

re-matches? checks if a regular expression matches a string (in this
case, a line from a file).

make-list-of-words-matching is used to loop over the lines of the
words file and return a list of lines matching the provided regular
expression.  Now I have the tools I need to make my word list.

\begin{verbatim}
(load-option 'format)

(define (re-matches? re line #!optional display-matches)
  ;; Regex String . Boolean -> Boolean
  "Attempt to match RE against LINE. Print the match if DISPLAY-MATCHES is set."
  (let ((match (re-string-match re line)))
    (if match
        (if (not (default-object? display-matches))
            (begin (format #t "|~A|~%" (re-match-extract line match 0))
                   #t)
            #t)
        #f)))

(define (make-list-of-words-matching re file)
  ;; Regex String -> List
  "Given a regular expression RE, loop over FILE, gathering matches."
  (call-with-input-file file
    (lambda (port)
      (let loop ((source (read-line port)) (sink '()))
        (if (eof-object? source)
            sink
            (loop (read-line port) (if (re-matches? re source)
                             (cons source sink)
                             sink)))))))
\end{verbatim}

\subsubsection{Writing tests}

Since I am not one of the 10\% of programmers who can implement a
correct binary search on paper, I started out by writing a test
procedure. The test procedure grew over time as I found bugs and read
an interesting discussion about the various edge cases a binary search
procedure should handle. These include:

\begin{itemize}
\item Empty list 
\item List has one word 
\item List has two word 
\item Word is not there and ``less than'' anything in the list 
\item Word is not there and ``greater than'' anything in the list 
\item Word is first item 
\item Word is last item 
\item List is all one word 
\end{itemize}

If multiple copies of word are in list, return the first word found
(this could be implemented to return the first or last duplicated
word)

Furthermore, I added a few ``sanity checks'' that check the return
values against known outputs. Here are the relevant procedures:

assert= checks two numbers for equality and prints a result

assert-equal checks two Scheme objects against each other with equal?
and prints a result

run-binary-search-tests reads in words from a file and runs all of our
tests

\begin{verbatim}
(define (assert= expected got #!optional noise)
  ;; Int Int -> IO
  (if (= expected got)
      (format #t "~A is ~A\t...ok~%" expected got)
      (format #t "~A is not ~A\t...FAIL~%" expected got)))

(define (assert-equal? expected got #!optional noise)
  ;; Thing Thing -> IO
  (if (equal? expected got)
      (format #t "~A is ~A\t...ok~%" expected got)
      (format #t "~A is not ~A\t...FAIL~%" expected got)))

(define (run-binary-search-tests)
  ;; -> IO
  "Run our binary search tests using known words from the 'words' file.
This file should be in the current working directory."
  (with-working-directory-pathname (pwd)
    (lambda ()
      (if (file-exists? "words")
          (begin
            (format #t "file 'words' exists, making a list...~%")
            (let* ((unsorted (make-list-of-words-matching "acc" "words"))
                   (sorted (sort unsorted string<?)))
              (format #t "doing binary searches...~%")
              (assert-equal? #f (binary-search "test" '())) ; empty list
              (assert-equal? #f (binary-search "aardvark" sorted)) ; element absent and too small
              (assert-equal? #f (binary-search "zebra" sorted)) ; element absent and too large
              (assert= 0 (binary-search "accusive" '("accusive"))) ; list of length one
              (assert= 0 (binary-search "acca" sorted)) ; first element of list
              (assert= 1 (binary-search "aardvark" '("aardvark" "aardvark" "babylon"))) ; multiple copies of word in list
              (assert= 1 (binary-search "barbaric" '("accusive" "barbaric"))) ; list of length two
              (assert= 98 (binary-search "acclamator" sorted))
              (assert= 127 (binary-search "aardvark" (map (lambda (x) "aardvark") test-list))) ; list is all one value
              (assert= 143 (binary-search "accomplice" sorted))
              (assert= 254 (binary-search "accustomedly" sorted))
              (assert= 255 (binary-search "accustomedness" sorted)))))))) ; last element of list
\end{verbatim}

\subsubsection{The binary search procedure}

Finally, here's the binary search procedure; it uses a couple of
helper procedures for clarity.

->int is a helper procedure that does a quick and dirty integer
conversion on its argument

split-difference takes a low and high number and returns the floor of
the halfway point between the two

binary-search takes an optional debug-print argument that I used a lot
while debugging. The format statements and the optional argument tests
add a lot of bulk --- now that the procedure is debugged, they can
probably be removed. (Aside: I wonder how much ``elegant'' code
started out like this and was revised after sufficient initial testing
and debugging?)

\begin{verbatim}
(define (->int n)
  ;; Number -> Int
  "Given a number N, return its integer representation.
N can be an integer or flonum (yes, it's quick and dirty)."
  (flo:floor->exact (exact->inexact n)))

(define (split-difference low high)
  ;; Int Int -> Int
  "Given two numbers, return their rough average."
  (if (= (- high low) 1)
      1
    (->int (/ (- high low) 2))))

(define (binary-search word xs #!optional debug-print)
  ;; String List -> Int
  "Do binary search of list XS for WORD. Return the index found, or #f."
  (if (null? xs)
      #f
    (let loop ((low 0) (high (- (length xs) 1)))
         (let* ((try (+ low (split-difference low high)))
                (word-at-try (list-ref xs try)))
           (cond
            ((string=? word-at-try word) try)
            ((< (- high low) 1) #f)
            ((= (- high try) 1) 
             (if (string=? (list-ref xs low) word)
                 low
               #f))
            ((string<? word-at-try word)
             (if (not (default-object? debug-print))
                 (begin (format #f "(string<? ~A ~A) -> #t~%try: ~A high: ~A low: ~A ~2%" %
                                 word-at-try word try high low)
                        (loop (+ 1 try) high)) ; raise the bottom of the window
                        (loop (+ 1 try) high)))
            ((string>? word-at-try word)
             (if (not (default-object? debug-print))
                 (begin (format #f "(string>? ~A ~A) -> #t~%try: ~A high: ~A low: ~A ~2%" %
                                 word-at-try word try high low)
                        (loop low (+ 1 try))) ; lower the top of the window
                        (loop low (+ 1 try))))
            (else #f))))))
\end{verbatim}

\subsubsection{Takeaways}

This exercise has taught me a lot. 

Writing correct code is hard. (I'm confident that this code is not
correct.) You need to figure out your invariants and edge cases
first. I didn't, and it made things a lot harder.

It's been said a million times, but tests are code. The tests required
some debugging of their own.

Once they worked, the tests were extremely helpful. Especially now
that I'm at the point where (if this were ``for real'') additional
features would need to be added, the format calls removed, the
procedure speeded up, and so on.

I hope this has been useful to some other aspiring Scheme wizards out
there. Happy Hacking!

\subsubsection{Binary search in words}

Binary search works like this:

1. Pick the element in the middle of the list.
2. Is it the word you're looking for?

If yes, you're done.

If no, check it against the element you're looking for:

If it's less than the element you're looking for:

Split the list in half at the current element

Search again, this time using only the high half of the list as
input

If it's greater than the element you're looking for:

split the list in half at the current element

search again, this time using only the low half of the list as
input

\subsubsection{Binary search in pictures}

%% TODO(rml): Add image showing how binary search works.

\section{Shuffling}

\chapter{Trees}

\section{Binary trees}

CSRMs: constructors, selectors, recognizers, and mutators.

Load the library:

\begin{verbatim}
> (load-module 'binary-tree)
\end{verbatim}

Basic operations:

\begin{itemize}
\item creation
\item insertion
\item updating (destructive/in-place)
\item deletion
\end{itemize}

Walking the tree using higher order functions (see notes from ADuni lectures).

\section{Balanced binary trees}

%% TODO(rml): Red-black tree or AVL tree? AVL is supposedly simpler to
%% implement but red-black is said to have superior tree rotation
%% runtime -- once we have a self-balancing tree of either type we can
%% write the "fast" treesort!

%% TODO(rml): Mention that when trees are balanced, then TREE-SORT can
%% be fast.  Add a link back to the TREE-SORT section from here.

\section{More on TREE-SORT}

%% TODO(rml): Mention tree-sort here, and note that this is only fast
%% if the tree is already balanced, so give the "slow version" first,
%% since balanced trees are not introduced yet. Explain why it can be
%% slow.

\section{Balancing Trees}

\section{Searching Trees}

\chapter{Graphs}

\section{Overview of Graphs}

%% TODO(rml): How to represent graphs with another data structure:
%% matrix, hash table, or association list. We might want to implement
%% our own hash tables first using balanced binary trees -- that would
%% be way cool!

%% In other words, it might be cool to build everything from the
%% bottom up, e.g.:

%% 1. Balanced binary tree

%% 2. Hash Table

%% 3. Graph (using hash table representation)

%% Discussion of common graph algorithms.

%% Traversal: TBD.

%% Search: Dijkstra's Algorithm.

%% Maybe include a discussion of 'longest-path.scm' (implementation is
%% currently incomplete)

\section{Traversal}

%% TODO(rml): Write up the =longest-path.scm= code here.

\section{Search}

%% TODO(rml): Add winston-horn-network.png image here.

I've been having fun translating some of the code in Winston and
Horn's \emph{Lisp} into Scheme.  This book is amazing --- clearly
written, with lots of motivating examples and applications.  As SICP
is to language implementation, \emph{Lisp} is to application
development, with chapters covering constraint propagation, forward
and backward chaining, simulation, object-oriented programming, and so
on.  And it does include the obligatory Lisp interpreter in one
chapter, if you're into that sort of thing.

In this installment, based on Chapter 19, we will look at some
simple strategies for searching for a path between two nodes on a
network (a graph).  The network we'll be using is shown in the
diagram above.

Here's the same network, represented as an alist where each
\verb|CAR:CDR| pair represents a \verb|NODE:NEIGHBORS| relationship:

\begin{verbatim}
'((f e)
  (e b d f)
  (d s a e)
  (c b)
  (b a c e)
  (a s b d)
  (s a d))
\end{verbatim}

The high-level strategy the authors use is to traverse the network,
building up a list of partial paths.  If a partial path ever reaches
the point where it describes a full path between the two network nodes
we're after, we've been successful.

As with trees, we can do either a breadth-first or depth-first
traversal.  Here's what the intermediate partial paths will look like
for a breadth-first traversal that builds a path between nodes
\verb|S| and \verb|F|:

\begin{verbatim}
(s)
(s a)
(s d)
(s a b)
(s a d)
(s d a)
(s d e)
(s a b c)
(s a b e)
(s a d e)
(s d a b)
(s d e b)
'(s d e f)
\end{verbatim}

Based on that output, we can deduce that every time we visit a node,
we want to extend our partial paths list with that node.  Here's one
option --- its only problem is that it will happily build circular
paths that keep us from ever finding the node we want:

\begin{verbatim}
(define (%buggy-extend path) ;; Builds circular paths
     (map (lambda (new-node)
            (cons new-node path))
          (%get-neighbor (first path))))
\end{verbatim}

(Incidentally, I've become fond of the convention whereby internal
procedures that aren't part of a public-facing API are prefixed with
the \verb|\%| character.  This can be found in some parts of the MIT
Scheme sources, and I believe it's used in Racket as well.  I've
started writing lots of my procedures using this notation to remind me
that the code I'm writing is not the real `API', that the design will
need more work, and that the current code is just a first draft.  I'm
using that convention here.)

Here's a better version that checks if we've already visited the node
before adding it to the partial paths list --- as a debugging aid it
prints out the current path before extending it:

\begin{verbatim}

(define (%extend path)
    (display (reverse path))
    (newline)
    (map (lambda (new-node)
           (cons new-node path))
         (filter (lambda (neighbor)
                   (not (member neighbor path)))
                 (%get-neighbor (first path)))))

\end{verbatim}

You may have noticed the \verb|\%GET-NEIGHBOR| procedure; it's just
part of some silly data structure bookkeeping code.  Please feel free
to deride me in the comments for my use of a global variable.  What
can I say?  I'm Scheming like it's 1988 over here!  Here's the
boilerplate:

\begin{verbatim}

(define *neighbors* '())

(define (%add-neighbor! k v)
  (let ((new-neighbor (cons k v)))
    (set! *neighbors*
          (cons new-neighbor *neighbors*))))

(define (%get-neighbor k)
  (let ((val (assoc k *neighbors*)))
    (if val
        (cdr val)
      '())))

(%add-neighbor! 's '(a d))
(%add-neighbor! 'a '(s b d))
(%add-neighbor! 'b '(a c e))
(%add-neighbor! 'c '(b))
(%add-neighbor! 'd '(s a e))
(%add-neighbor! 'e '(b d f))
(%add-neighbor! 'f '(e))

\end{verbatim}

Now that we have our data structure and a way to extend our partial
path list (non-circularly), we can write the main search procedure,
\verb|\%BREADTH-FIRST|.  The authors have a lovely way of explaining
its operation:

\begin{quote}
\verb|BREADTH-FIRST| is said to do a breadth-first search because it
extends all partial paths out to uniform length before extending any
to a greater length.
\end{quote}

Here's the code, translated to use a more Schemely, iterative named
\verb|LET| instead of the linear-recursive definition from the book:

\begin{verbatim}

(define (%breadth-first start finish network)
  (let ((queue (list (list start))))
    (let loop ((start start)
               (finish finish)
               (network network)
               (queue queue))
      (cond ((null? queue) '())         ;Queue empty?
            ((equal? finish (first (first queue))) ;Finish found?
             (reverse (first queue)))              ;Return path.
            (else
             (loop start
                   finish               ;Try again.
                   network
                   (append
                    (rest queue)
                    (extend (first queue))))))))) ;New paths in front.

\end{verbatim}

(A better way to write this procedure would be to implement a generic
internal search procedure that takes its `breadthiness' or
`depthiness' as a parameter.  We could then wrap it with nicer
public-facing search procedures specific names.)

Meanwhile, back at the REPL, we remind ourselves of what
\verb|*NEIGHBORS*| actually looks like, and then we search for a path
between the nodes \verb|S| and \verb|F|.

\begin{verbatim}

     > *neighbors*
     '((f e) (e b d f) (d s a e) (c b) (b a c e) (a s b d) (s a d))
     > (%breadth-first 's 'f *neighbors*)
     (s)
     (s a)
     (s d)
     (s a b)
     (s a d)
     (s d a)
     (s d e)
     (s a b c)
     (s a b e)
     (s a d e)
     (s d a b)
     (s d e b)
     '(s d e f)

\end{verbatim}

What fun!  I can almost imagine using a three-dimensional variant of
these searches for a space wargame with wormhole travel!  Except, you
know, they'd need to be much faster and more skillfully implemented.
There's also the tiny requirement to write the surrounding game.

%% TODO(rml): Is this a footnote?
It shouldn't need to be said, but: Of course the authors knew better;
they were trying to hide that unnecessary complexity from you until
later.

\section{Graph coloring}

\section{Finding the shortest path between nodes}

\chapter{Strings}

%% TODO(rml): Figure out what the (say) 2-3 most basic algorithms are
%% that we need to cover.

\part{Variations}

%% TODO(rml): In Part II we will implement variations on the simpler
%% algorithms from Part I. (Aside: I'm not sure about this section yet.

\part{The Real World}
\chapter{A hash table library}

In this chapter, we're going to implement our own hash tables.

In day-to-day programming, the hash table is probably the most
important real-world data structure.

The hash table also gives us a nice real-world proving ground for our
algorithms skills, since implementing hash tables requires putting
together several different data structures into one --- in other
words, it is a \emph{compound data structure}.

%% TODO(rml): Write this chapter.

\chapter{A regular expression library}

In this chapter, we're going to implement our own regular expression
matching library.

%% TODO(rml): Write this chapter.

\chapter{Glossary}

%% TODO(rml): Figure out how to translate this section to LaTeX world.

\section{Iterative process}

In terms of Scheme code, code that describes an iterative process
usually looks like this:

\begin{verbatim}
    (define (+ a b)
      (if (= a 0)
          b
          (+ (decr a)
             (incr b))))
\end{verbatim}

You can visualize the operation of an iterative process like this:

\begin{verbatim}
    (+ 4 3)
    (+ 3 4)
    (+ 2 5)
    (+ 1 6)
    (+ 0 7)
    7
\end{verbatim}

Notice how the ``shape'' of the successive calls to $+$ stays the same
``size''?  In other words, it doesn't grow out to the right.

Using Big O notation [3], you can say that $+$ uses $O(1)$ space
(memory), and $O(n)$ time (CPU).

\section{Recursive Process}

A recursive process is one that consumes growing amounts of stack
space while it runs.  It terms of Scheme code, code that describes a
recursive process usually looks like this:

\begin{verbatim}

(define (incr n) (+ n 1))
(define (decr n) (- n 1))

(define (+ a b)
  (if (= a 0)
      b
      (incr (+ (decr a) b))))

\end{verbatim}

You can visualize the operation of a recursive process like this:

\begin{verbatim}

(+ 3 4)
(incr (+ (decr 3) 4))
(incr (incr (+ (decr 2) 4)))
(incr (incr (incr (+ (decr 1) 4))))
(incr (incr (incr 4)))
7

\end{verbatim}

Notice how the ``shape'' of the successive calls to $+$ get
``larger''?

Using Big O notation, you can say that $+$ uses $O(n)$ time in its
first argument and $O(n)$ space in its second.

\section{Big O Notation}

``Big O'' notation is a way to talk about the resource usage of an
algorithm.  This usage can be along several axes:

\begin{enumerate}
\item Time (CPU --- how many instructions will it take to compute?)
\item Space (Memory --- how much storage will it use?)
\end{enumerate}

Instead of ``resource usage'' you can also say the ``complexity'' of
the algorithm.  This is the term you are likely to find in more
academic writings.

To be more precise, this notation describes the upper bound of the
resource usage.  This means that, even in the worst case scenario, the
algorithm will not use more than a given amount of resources.

For more details, check out the following references:

\begin{itemize}
\item \url{https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation}

\item \url{http://stackoverflow.com/questions/487258/what-is-a-plain-english-explanation-of-big-o-notation}

\item \url{http://bigocheatsheet.com/}
\end{itemize}

The last page has a nice big graph that makes it easy to visualize the
different complexities.  Further down the page there are tables that
list algorithm complexities for various operations (insert, delete,
search) on data structures such as stacks, lists, hash tables, etc.
Add this to your bookmarks so you can refer back to it as needed.

\appendix
\chapter{Loading the book code into a Scheme}

%% TODO(rml): Write instructions for loading the book code into each
%% of the supported Schemes.

\backmatter{}
\chapter{Bibliography}

\begin{itemize}

\item Abelson \& Sussman, \emph{Structure and Interpretation of Computer Programs}, 1st ed., 1986.

\item Winston \& Horn, \emph{Lisp}, 198?.

\item Gabriel, \emph{Performance and Evaluation of Lisp Systems}, 1985.

\item ???, \emph{Compared to What?}, ???.

\end{itemize}

\end{document}
