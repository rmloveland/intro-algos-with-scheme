* An Invitation to Algorithms with Scheme

by [Richard M. Loveland|mailto:r@rmloveland.com]

 {toc:minlevel=2|maxlevel=2}

** Preface

Why another book about algorithms?

There are already many books about algorithms.  Most provide detailed analyses of topics such as "Big O notation" and are aimed at an academic audience.  As a result, they strive to be comprehensive, and they may run to hundreds of pages in length.

This book is different because its goals are different.  First, it is an _invitation_ to algorithms; this means that we will cover one or two common algorithms in each of the following areas:

+ Lists
+ Trees
+ Graphs
+ Strings

You can get surprisingly far with these simple building blocks!

Further, it is an invitation to algorithms _with Scheme_.  This means that we will use Scheme to describe the algorithms we want the computer to perform.  It is my opinion that the notation provided by Scheme provides a clear description of many common algorithms.

Once we've covered the basics, we will work on some interesting projects, such as building our own hash tables.  This will take us out of the realm of "textbook" algorithms and into the messy but fun world of real programming.  After all, programming isn't much fun unless you can get the computer to do stuff you actually care about.

** Introduction

What is an algorithm?

An _algorithm_ is a recipe for getting a job done by the computer.  Many algorithms are very simple, since many of the jobs we need a computer to do are simple.  For example, we may need to sort a list of numbers.

"Sort a list of numbers" sounds simple, but there is some complexity there that needs to be addressed.  How long is the list?  Is it longer than the available memory on our machine?  Are the numbers in the list in random order?  Or are the numbers almost sorted already?

Sometimes, depending on the answers to these questions, you'll need to choose one sorting algorithm or another.  Some take more memory (space).  Some require more CPU cycles (time).

Unless you are doing very specialized optimization work, you can probably get along very well with just knowing two or three of the most common sorting algorithms, and choosing which one to use based on the needs of the problem at hand.

And that brings us to the point of this book.  We will study two or three common algorithms across each of several common data structures.  Then, we will implement these algorithms in the Scheme programming language.  As our mastery increases, we will take on several "Real World" implementation projects, such as writing our own hash tables and our own regular expression matcher.

Along the way, we will observe several themes that repeat:

1. _Divide and conquer_: break the problem into pieces, solve a smaller problem.
2. _Recursion_: Do it again, and again, and again, until you're done.

** Prerequisites

 *Familiarity with Scheme*

This book does not try to teach the Scheme language.  We assume you have already encountered an introduction to Scheme elsewhere. In particular, you should be pretty comfortable with recursion, since we'll be using plenty of it.  By recursion, we just mean that you understand code that looks like this:

{code}
(define (+ a b)
  (if (= a 0)
      b
      (+ (decr a)
         (incr b))))
{code}

A good book that serves this purpose is [The Little Schemer|https://www.amazon.com/Little-Schemer-Daniel-P-Friedman/dp/0262560992/ref=sr_1_1?ie=UTF8&qid=1489850762&sr=8-1&keywords=the+little+schemer], by Dan Friedman & Matthias Felleisen.  For more recommendations, see the [Bibliography|#Bibliography-TOC].

 *Access to a computer with a Scheme Interpreter*

This book's code has been tested with the following combinations of Scheme implementation and operating system:

{table}
|| Implementation || OS ||
| Larceny | Linux, Windows |
| Chez (Petite) | Windows |
| Gambit | Windows, Linux |
{table}

** Conventions

Procedure names are written inline with the text like this: {{CHAR-READY?}}

Code samples are set off from the surrounding text like this:

{code}
(define (double n)
  (+ n n))
{code}

** 1. Lists

The list data structure is ubiquitous in Scheme programming.  One of the most common patterns is to gather up a list of elements, and then process them.

When we say that something is a "list" in Scheme, we mean that it's actually a _linked list_, which has the following structure:

{code}

 ---    ---          ---   ---
| CAR | CDR | ---> | CAR | CDR | ---> NIL
 ---    ---          ---   ---
  |                   |
 'Apple            'Banana

{code}

As you can see from the diagram, each element of the linked list has two "cells":

1. The first cell holds the contents of that element, e.g., {{'Apple}}
2. The second cell points to the next element of the list

In code, we'd write this as just: {{'(Apple Banana)}}.  Or more verbosely,

{code}
(cons 'Apple (cons 'Banana '()))
{code}

In other languages we'd need to represent this linked list structure explicitly, whereas in Scheme it's built-in.

In the sections that follow we'll implement common operations on lists.

+ [Sorting|#Sorting-TOC]
+ [Searching|#Searching-TOC]
+ [Updating|#Updating-TOC]

*** Sorting

In this section we'll implement the following sort algorithms for lists:

+ [Merge Sort|#Merge Sort-TOC]
+ [Quick Sort|#Quick Sort-TOC]
+ [Tree Sort|#Tree Sort-TOC]

If you can understand the sort implementations described in this chapter, you should have no problem learning more about (and implementing your own versions of) other sorting algorithms that you encounter.

**** Merge Sort

There are a number of good algorithms for sorting lists.  We'll start with [Merge Sort|https://en.wikipedia.org/wiki/Merge_sort] for the following reasons:

1. It performs well, and provides a good tradeoff between implementation complexity, performance, and CPU and memory usage
2. It is a good example of the "divide and conquer" strategy for designing algorithms
3. It suits Scheme (and other Lisps) due to the way (linked) lists are implemented in the language

In this section we'll look at several implementations of merge sort, starting with the naive recursive version you can easily find on the internet, followed by an [iterative|#glossary-iterative-process] implementation.

***** The architecture of a merge sort implementation

Merge sort is most easily implemented with a two-part architecture:

1. {{MERGE}}, a lower-level "helper" procedure that merges two partially sorted lists
2. {{MERGE-SORT}}, a top-level "driver" procedure that internally calls {{MERGE}} repeatedly to get the list sorted

Let's look at each of these procedures in turn.

{{MERGE}} handles splicing two sublists together in an ordering determined by a [predicate|https://people.eecs.berkeley.edu/~bh/ssch6/true.html] such as {{<}}.

In other words, assuming we use {{<}} as our predicate, {{MERGE}} walks both lists, putting things in pairwise order (in this case numbers) based on which of them are "less than" the other.

It doesn't fully sort the lists; it only works on one item from each list at a time.  That's why we need the {{MERGE-SORT}} driver procedure.

Here are examples showing some expected inputs and outputs:

{code}
> (merge < '(2 191) '(18 45))
(2 18 45 191)

> (merge string<? '("scheme" "hacking") '("is" "fun"))
("is" "fun" "scheme" "hacking")
{code}

The actual {{MERGE-SORT}} procedure can be thought of as a "driver" for {{MERGE}}.  It handles calling {{MERGE}} repeatedly and splicing the partially sorted lists returned by successive calls to {{MERGE}} together into a final list that is fully sorted.

Here are examples showing some expected inputs and outputs:

{code}
> (merge-sort '(17 51 55 13 12 75 98 48 98 89 68 86 89 51) <)
(12 13 17 48 51 51 55 68 75 86 89 89 98 98)

> (merge-sort '("i" "am" "a" "little" "teapot") string<?)
("a" "am" "i" "little" "teapot")
{code}

***** How {{MERGE}} Works

{{MERGE}} splices two lists together in a pairwise ordering determined by a predicate such as {{<}}.

It walks two lists, call them _A_ and _B_, comparing the elements of each list in turn.  It keeps another list, {{RESULT}}, where it stores the output.  If the current element from _A_ is less than the current element from _B_, {{MERGE}} pushes it onto the output list.  Otherwise, it pushes the current element from _B_ onto the output list.

Once {{MERGE}} has traversed both of its input lists _A_ and _B_, and processed all the elements of each, it returns its result.

As noted above, the output of {{MERGE}} is *not* a sorted list.  It has been put into "pairwise order".  This is a fancy way of saying that we only compared two elements at a time, one each from _A_ and _B_, as we were building it.

***** Implementing {{MERGE}}

Let's begin implementing {{MERGE}} by reminding ourselves of its specification.  It takes as its arguments a predicate and two lists, and it returns a "pairwise ordered" list.

The following code shows a way of annotating our program code that serves as a form of documentation.  It makes it a little easier to remember what a procedure does when you've been away from it for a while.

{code}
(define (merge pred A B)
  ;; Pred List List -> List
  (let ((return '()))
    return))
{code}

This meets the spec, in that it takes the right arguments and returns a list, but it doesn't actually do anything.

Based on our above description of the merge algorithm, the implementation is straightforward, if a bit long:

YOU ARE HERE

{code}
(define (merge pred l r)
  (letrec ((merge-aux
	    (lambda (pred left right result)
	      (cond 
	       ;; If LEFT and RIGHT are both numbers, listify them so
	       ;; MERGE-AUX can work with them.
	       ((and (number? left)
		     (number? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If LEFT is just a number, listify it so MERGE-AUX
	       ;; can work with it.
	       ((number? left)
		(merge-aux pred (list left) right result))

	       ;; Likewise, if RIGHT is just a number, listify it for
	       ;; MERGE-AUX.
	       ((number? right)
		(merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are both strings, listify them so
	       ;; MERGE-AUX can use them.
	       ((and (string? left)
		     (string? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If either LEFT or RIGHT are just strings, listify
	       ;; for MERGE-AUX.
	       ((string? left) (merge-aux pred (list left) right result))
	       ((string? right) (merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are empty, we're done merging.
	       ;; Return the result.
	       ((and (null? left)
		     (null? right))
		(reverse result))

	       ;; If LEFT and RIGHT still have elements to be
	       ;; processed, call PRED and run them through MERGE-AUX
	       ;; again.
	       ((and (not (null? left))
		     (not (null? right)))
		(if (pred (car left)
			  (car right))
		    (merge-aux pred
			       (cdr left)
			       right
			       (cons (car left) result))
		  (merge-aux pred
			     left
			     (cdr right)
			     (cons (car right) result))))

	       ;; If the cases above haven't matched, and LEFT is not
	       ;; NULL?, I call myself.
	       ((not (null? left))
		(merge-aux pred (cdr left) right (cons (car left) result)))

	       ;; Same as the previous case -- this time with RIGHT.
	       ((not (null? right))
		(merge-aux pred left (cdr right) (cons (car right) result)))

	       ;; We should never get here.
	       (else #f)))))
    (merge-aux pred l r '())))
{code}

Let's go through the COND clause-by-clause.  First, as our recursive base case,

***** Recursive Merge Sort

***** Bottom-up Merge Sort

We've already done the hard part by writing {{MERGE}}.  Now we just need to write the "driver" procedure that will call it.  Hence, {{MERGE-SORT}}.  Take a few minutes to study the commented version:

{code}
(define (merge-sort xs pred)
  (let loop ((xs xs)
	     (result '()))
       (cond ((and (null? xs)
		   (null? (cdr result)))
	      (car result))
	     ((null? xs)
	      (loop result xs))
	     ((null? (cdr xs))
	      (loop (cdr xs)
		    (cons (car xs) result)))
	     (else
	      (loop (cddr xs)
		    (cons (merge <
				 (first xs)
				 (second xs))
			  result))))))
{code}

**** Quick Sort

**** Tree Sort

*** Searching

Now that we've sorted a list of elements, we can search it.  It turns out that searching through a list of things is much faster if you can sort that list first.

In this section we'll look at a particular type of search algorithm called /binary search/.  Binary search is so named because it cuts the search space in half with every iteration.

Unlike some other searches, binary search only works on ordered lists of things.  That is why we had to go through the trouble of sorting our list earlier: so that we could search through it now.

***** Binary search in words

Binary search works like this:

1. Pick the element in the middle of the list.
2. Is it the word you're looking for?

If yes, you're done.

If no, check it against the element you're looking for:

If it's _less than_ the element you're looking for:

Split the list in half at the current element

Search again, this time using only the "high half" of the list as input

If it's _greater than_ the element you're looking for:

split the list in half at the current element

search again, this time using only the "low half" of the list as input

***** Binary search in pictures

INSERT IMAGE (GIF?) OF BINARY SEARCH ALGO HERE.

***** Properties of Binary Search

***** When to use binary search

*** Updating

** 2. Trees

*** Binary trees

CSRMs: constructors, selectors, recognizers, and mutators.

 *Basic operations*

+ creation
+ insertion
+ updating (destructive/in-place)
+ deletion

Walking the tree using higher order functions (see notes from ADuni lectures).

 *Sorting with treesort*

This is only fast if the tree is balanced, so give the "slow version" first, since balanced trees are not introduced yet. Explain why it can be slow.

 *Balanced binary trees*

Red-black tree or AVL tree? AVL is supposedly simpler to implement but red-black is said to have superior tree rotation runtime -- once we have a self-balancing tree of either type we can write the "fast" treesort!

**** Balancing Trees

**** Searching Trees

** 3. Graphs

*** Overview of Graphs

How to represent graphs with another data structure: matrix, hash table, or association list. We might want to implement our own hash tables first using balanced binary trees -- that would be way cool!

In other words, it might be cool to build everything from the bottom up, e.g.:

1. Balanced binary tree
2. Hash Table
3. Graph (using hash table representation)

Discussion of common graph algorithms.

Traversal: TBD.

Search: Dijkstra's Algorithm.

Maybe include a discussion of {{longest-path.scm}} (implementation is currently incomplete)

*** Traversal

Write up the {{longest-path.scm}} code here.

*** Search

!winston-horn-network.png:520!

I've been having fun translating some of the code in Winston and Horn's _Lisp_ into Scheme.  This book is amazing -- clearly written, with lots of motivating examples and applications.  As SICP is to language implementation, _Lisp_ is to application development, with chapters covering constraint propagation, forward and backward chaining, simulation, object-oriented programming, and so on.  And it does include the obligatory Lisp interpreter in one chapter, if you're into that sort of thing.

In this installment, based on Chapter 19, we will look at some simple strategies for searching for a path between two nodes on a network (a graph).  The network we'll be using is shown in the diagram above.

Here's the same network, represented as an alist where each {{CAR:CDR}} pair represents a {{NODE:NEIGHBORS}} relationship:

{code}
  '((f e)
    (e b d f)
    (d s a e)
    (c b)
    (b a c e)
    (a s b d)
    (s a d))
{code}

The high-level strategy the authors use is to traverse the network, building up a list of partial paths.  If a partial path ever reaches the point where it describes a full path between the two network nodes we're after, we've been successful.

As with trees, we can do either a breadth-first or depth-first traversal.  Here's what the intermediate partial paths will look like for a breadth-first traversal that builds a path between nodes {{S}} and {{F}}:

{code}
  (s)
  (s a)
  (s d)
  (s a b)
  (s a d)
  (s d a)
  (s d e)
  (s a b c)
  (s a b e)
  (s a d e)
  (s d a b)
  (s d e b)
  '(s d e f)
{code}

Based on that output, we can deduce that every time we visit a node, we want to extend our partial paths list with that node.  Here's one option -- its only problem is that it will happily build circular paths that keep us from ever finding the node we want:

{code}
  (define (%buggy-extend path)             ;builds circular paths
    (map (lambda (new-node) 
           (cons new-node path))
         (%get-neighbor (first path))))
{code}

(Incidentally, I've become fond of the convention whereby internal procedures that aren't part of a public-facing API are prefixed with the `%' character.  This can be found in some parts of the MIT Scheme sources, and I believe it's used in Racket as well.  I've started writing lots of my procedures using this notation to remind me that the code I'm writing is not the real `API', that the design will need more work, and that the current code is just a first draft.  I'm using that convention here.)

Here's a better version that checks if we've already visited the node before adding it to the partial paths list -- as a debugging aid it prints out the current path before extending it:

{code}
  (define (%extend path)
    (display (reverse path))
    (newline)
    (map (lambda (new-node)
           (cons new-node path))
         (filter (lambda (neighbor)
                   (not (member neighbor path)))
                 (%get-neighbor (first path)))))
{code}

You may have noticed the {{%GET-NEIGHBOR}} procedure; it's just part of some silly data structure bookkeeping code.  Please feel free to deride me in the comments for my use of a global variable.  What can I say?  I'm Scheming like it's 1988 over here!  Here's the boilerplate:

{code}
  (define *neighbors* '())
  
  (define (%add-neighbor! k v)
    (let ((new-neighbor (cons k v)))
      (set! *neighbors*
            (cons new-neighbor *neighbors*))))
  
  (define (%get-neighbor k)
    (let ((val (assoc k *neighbors*)))
      (if val
          (cdr val)
          '())))
  
  (%add-neighbor! 's '(a d))
  (%add-neighbor! 'a '(s b d))
  (%add-neighbor! 'b '(a c e))
  (%add-neighbor! 'c '(b))
  (%add-neighbor! 'd '(s a e))
  (%add-neighbor! 'e '(b d f))
  (%add-neighbor! 'f '(e))
{code}

Now that we have our data structure and a way to extend our partial path list (non-circularly), we can write the main search procedure, {{%BREADTH-FIRST}}.  The authors have a lovely way of explaining its operation:

{quote}
BREADTH-FIRST is said to do a "breadth-first" search because it extends all partial paths out to uniform length before extending any to a greater length.
{quote}

Here's the code, translated to use a more Schemely, iterative named {{LET}} instead of the linear-recursive definition from the book:

{code}
  (define (%breadth-first start finish network)
    (let ((queue (list (list start))))
      (let loop ((start start)
                 (finish finish)
                 (network network)
                 (queue queue))
        (cond ((null? queue) '())                    ;Queue empty?
              ((equal? finish (first (first queue))) ;Finish found?
               (reverse (first queue)))              ;Return path.
              (else
               (loop start
                     finish               ;Try again.
                     network
                     (append
                      (rest queue)
                      (extend (first queue))))))))) ;New paths in front.
{code}

(A better way to write this procedure would be to implement a generic internal search procedure that takes its `breadthiness' or `depthiness' as a parameter.  We could then wrap it with nicer public-facing search procedures specific names.)

Meanwhile, back at the REPL... we remind ourselves of what {{*NEIGHBORS*}} actually looks like, and then we search for a path between the nodes {{S}} and {{F}}.

{code}
> *neighbors*
'((f e) (e b d f) (d s a e) (c b) (b a c e) (a s b d) (s a d))
> (%breadth-first 's 'f *neighbors*)
(s)
(s a)
(s d)
(s a b)
(s a d)
(s d a)
(s d e)
(s a b c)
(s a b e)
(s a d e)
(s d a b)
(s d e b)
'(s d e f)
{code}

What fun!  I can almost imagine using a three-dimensional variant of these searches for a space wargame with wormhole travel!  Except, you know, they'd need to be much faster and more skillfully implemented.  There's also the tiny requirement to write the surrounding game...

It shouldn't need to be said, but: Of course the authors knew better; they were trying to hide that unnecessary complexity from you until later.

** 4. Strings

** 5. (RW) Hash Tables

In this chapter, we're going to implement our own hash tables.

In day-to-day programming, the hash table is probably the most important real-world data structure.

The hash table also gives us a nice real-world proving ground for our algorithms skills, since implementing hash tables requires putting together several different data structures into one - in other words, it is a _compound data structure_.

** 6. (RW) Regular Expressions

** 7. (RW) Neural Networks

** Glossary

<a name="glossary-iterative-process" />

*** Iterative process

An iterative process is one that does not consume growing amounts of stack space while it runs.  In terms of Scheme code, code that describes an iterative process usually looks like this:

{code}
(define (+ a b)
  (if (= a 0)
      b
      (+ (decr a)
         (incr b))))
{code}

You can visualize the operation of an iterative process like this:

{code}
(+ 4 3)
(+ 3 4)
(+ 2 5)
(+ 1 6)
(+ 0 7)
;; => 7
{code}

Notice how the "shape" of the successive calls to {{+}} stays the same "size"?  In other words, it doesn't grow out to the right.

Using [Big O notation|#glossary-big-o-notation], you can say that {{+}} uses _O(1)_ space (memory), and _O(n)_ time (CPU).

*** Recursive Process

<a name="glossary-recursive-process" />

A recursive process is one that consumes growing amounts of stack space while it runs.  It terms of Scheme code, code that describes a recursive process usually looks like this:

{code}
(define (incr n) (+ n 1))
(define (decr n) (- n 1))

(define (+ a b)
  (if (= a 0)
      b
      (incr (+ (decr a) b))))
{code}

You can visualize the operation of a recursive process like this:

{code}
(+ 3 4)
(incr (+ (decr 3) 4))
(incr (incr (+ (decr 2) 4)))
(incr (incr (incr (+ (decr 1) 4))))
(incr (incr (incr 4)))
7
{code}

Notice how the "shape" of the successive calls to {{+}} get "larger"?

Using [Big O notation|#glossary-big-o-notation], you can say that {{+}} uses _O(n)_ time in its first argument and _O(n)_ space in its second.

*** Big O Notation

<a name="glossary-big-o-notation" />

"Big O" notation is a way to talk about the resource usage of an algorithm.  This usage can be along several axes:

+ Time (CPU - how many instructions will it take to compute?)
+ Space (Memory - how much storage will it use?)

Instead of "resource usage" you can also say the "complexity" of the algorithm.  This is the term you are likely to find in more academic writings.

To be more precise, this notation describes the "upper bound" of the resource usage.  This means that, even in the worst case scenario, the algorithm will not use more than a given amount of resources.

For more details, check out the following references:

+ [A beginner's guide to Big O notation|https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/]: Nice short article to get your feet wet with.

+ [What is a plain English explanation of "Big O notation?|http://stackoverflow.com/questions/487258/what-is-a-plain-english-explanation-of-big-o-notation] - the first answer provides a really good overview.  If you don't understand some of the mathier parts, just skip over them.

+ [Big-O Cheat Sheet|http://bigocheatsheet.com/]: This page has a nice big graph that makes it easy to visualize the different complexities.  Further down the page there are tables that list algorithm complexities for various operations (insert, delete, search) on data structures such as stacks, lists, hash tables, etc.  Add this to your bookmarks so you can refer back to it as needed.

** Bibliography

+ Abelson & Sussman, _Structure and Interpretation of Computer Programs_, 1st ed.

+ Winston & Horn, _Lisp_, X ed.

** Glossary

** Appendices

*** Appendix A. Loading code into your Scheme

Describe the process of loading the book's code here.
