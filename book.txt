h1. An Invitation to Algorithms with Scheme

by [Richard M. Loveland|mailto:r@rmloveland.com]

*Table of Contents*
{toc:minlevel=2|maxlevel=2}

h2. Preface

h3. Why another book about algorithms?

There are many books about algorithms.  Most provide detailed analyses of topics such as ``big O notation'' and are aimed at an academic audience.  As a result, they strive to be comprehensive, and in so doing, run to hundreds of pages in length.

This book is different, because its goals are different.  First, it is an *invitation* to algorithms; this means that we will cover one or two common algorithms in each of several areas: searching, sorting, trees, graphs, and matrices.  You can get surprisingly far with these simple building blocks.

Further, it is an invitation to algorithms *with Scheme*.  This means that we will use Scheme to describe the algorithms we want the computer to perform.  Along the way, we will solve real problems: disk I/O, parsing input file formats, and so on.  This will take us out of the realm of "pure" textbook algorithms and into the messy but fun world of real programming.  Because programming isn't much fun unless you can get the computer to actually do stuff.

Finally, it is my opinion that the notation provided by Scheme provides a clear description of many common algorithms.  It is even thought of by some programmers as beautiful, but I leave that for you to decide.

h3. Prerequisites

The following are the recommended prerequisites for reading this book:

Familiarity with Scheme:

This book does not try to teach the Scheme language.  We assume the reader has already encountered an introduction to Scheme elsewhere. In particular, you should be pretty comfortable with recursion.  A good book that serves this purpose is _The Little Schemer_, by Dan Friedman & Matthias Felleisen.  For a more detailed introduction to the Scheme language, see Dybvig's excellent _The Scheme Programming Language_.

The Scheme 48 Interpreter:

You will need a copy of Scheme 48, version 0.53.  It is highly likely that you will need access to a computer (or virtual machine) running Mac OS X or Linux to build it.

h3. Conventions

Procedure names are written like this: {{CHAR-READY?}}

Code samples are set off from the surrounding text like so:

{code}
(define (foo n)
  (expt n n))
{code}

h2. Introduction

What is an algorithm?

The three (or so) big ideas of this book:

1. Divide and conquer
2. Recursion
3. ???

Overview of the chapters.

Testing library.

Utilities library.

Discussion of type annotations and return values in procedure
definitions. Facilitates skim-reading code.

Building a tags file for source navigation.

Edwin basics.

The `pp' procedure.

The `show-parameter-list' procedure (M-Shift-A).

The `scheme-complete-variable' procedure (ESC-TAB).

The debugger.

h2. Lists

h3. Sorting

Although ostensibly, computers gonna compute, it turns out that most of the time we use them for their data processing capabilities more than as pure calculators.  A lot of what we do with computers (some would argue almost all of it) has to do with shuffling data around from one place to the next (or from one format into the next).  As a result, computers are beginning to become external brains that we dump things into for later use.

As external brains, one of the most important things computers help us with is in finding things.  As it happens, it's much easier to search for and find things if you put them in order first.  This is as true for computers as it is for your house.

That's why we'll start with sorting.

There are a number of different sorting algorithms with different properties.  In this book we'll focus on [Merge Sort|https://en.wikipedia.org/wiki/Merge_sort].  How come? It has a few qualities that make it worth using: 

1. It performs reasonably well, and provides a good tradeoff between implementation complexity and overall performance.
2. It is a good example of the general "divide and conquer" strategy for designing algorithms.
3. It suits Scheme due to the way lists (cons cells, really) are implemented.

In this chapter we will look at several implementations of merge sort, starting with the naive tree-recursive version you can easily find on the internet, and finishing up with a relatively performant iterative[1] implementation.

If you can understand the implementation of merge sort described in this chapter, you should have no problem learning more about (and implementing your own versions of) quicksort, heap sort, and the like that you come across in other books or on the web.

h4. Architecture of a Merge Sort Implementation

Merge sort is usually implemented with an architecture that consists of two parts: {{MERGE}}, a lower-level "helper" procedure that merges two partially sorted lists, and {{MERGE-SORT}}, a top-level procedure that internally calls {{MERGE}} repeatedly to get the list sorted.

Let's look at each of these procedures in turn.

{{MERGE}} handles splicing two sublists together in an ordering determined by a predicate such as {{<}}.  In other words, it puts things in order based on which of them are "less than" the other (this can have different meanings for numbers and strings).  Here are some examples showing the expected output.

{code}
(merge < '(2 191) '(18 45))
;; => (2 18 45 191)

(merge string<? '("scheme" "hacking") '("is" "fun"))
;; => ("is" "fun" "scheme" "hacking")
{code}

{{MERGE-SORT}} is a higher-level procedure that we can think of as a "driver" for {{MERGE}}.  It handles calling {{MERGE}} repeatedly and splicing the successively larger partially sorted lists returned by calls to {{MERGE}} together into one that is fully sorted.  Here are some examples showing its expected output:

{code}
(merge-sort '(17 51 55 13 12 75 98 48 98 89 68 86 89 51 73 18 92) <)
;; => (12 13 17 18 48 51 51 55 68 73 75 86 89 89 92 98 98)

(merge-sort '("i" "am" "a" "little" "teapot" "short" "and" "stout") string<?)
;; => FIXME: MERGE-AUX broken for strings, something is assuming numbers.
{code}

h4. How MERGE Works

As we just saw, before we can do a merge sort, we need to write the =MERGE}} procedure.  Remember that it splices two lists together in an ordering determined by a predicate such as {{<}}.  (For expository purposes, we will stick to numbers for the rest of this implementation.)

An easy way to do this work is to walk two lists, /A/ and /B/, comparing the elements of each list in turn.  We will keep another list, {{RESULT}}, where we will put our, er, result.  If we assume =i}} is the current position in the list, and if {{(list-ref A i)}} (the current element from A) is less than {{(list-ref B i)}} (the current element from B), we push {{(list-ref A i)}} onto the results list.  Otherwise, we push {{(list-ref B i)}} onto the results list.

IMAGE

Finally, having traversed both of our input lists A and B, and processed all the elements of each, we return {{RESULT}}.

IMAGE

It's important to note that {{RESULT}} is *not* a sorted list.  It has been put into "pairwise order".  This is a fancy way of saying that we only compared two elements at a time, one each from /A/ and /B/, as we were building it.

At this point, you can probably begin to see why {{MERGE-SORT}} calls =MERGE}} over and over when it's sorting a list.

h4. Implementing MERGE

Let's begin implementing MERGE by reminding ourselves of its specification.  MERGE takes as its arguments a predicate and two lists, and it returns a `pairwise ordered' list.  Another way of writing this is as follows (you may have noticed this notation above):

{code}
; MERGE : Pred List List -> List
{code}

This is just a way of annotating our program code that serves as a form of documentation.  It makes it a little easier to remember what a procedure does when you've been away from it for a while. Given this spec, we can start our implementation:

{code}
; MERGE : Pred List List -> List
(define (merge pred A B)
  (let ((return '()))
    return))
{code}

This meets the spec, in that it takes the right arguments and returns a list, but it doesn't actually do anything.  In fact, because we already know we're going to be returning a list, let's write it in a recursive style that uses an explicit list argument. This explicit list will be the value of RETURN we talked about earlier.  It also means we need to update our spec slightly.

{code}
; MERGE : Pred List List List -> List
(define (merge pred A B RETURN)
  RETURN)
{code}

Based on our above descriptions of the algorithm, the implementation of MERGE is straightforward, if a bit long:

{code}
(define (merge pred l r)
  (letrec ((merge-aux
	    (lambda (pred left right result)
	      (cond 
	       ;; If LEFT and RIGHT are both numbers, listify them so
	       ;; MERGE-AUX can work with them.
	       ((and (number? left)
		     (number? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If LEFT is just a number, listify it so MERGE-AUX
	       ;; can work with it.
	       ((number? left)
		(merge-aux pred (list left) right result))

	       ;; Likewise, if RIGHT is just a number, listify it for
	       ;; MERGE-AUX.
	       ((number? right)
		(merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are both strings, listify them so
	       ;; MERGE-AUX can use them.
	       ((and (string? left)
		     (string? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If either LEFT or RIGHT are just strings, listify
	       ;; for MERGE-AUX.
	       ((string? left) (merge-aux pred (list left) right result))
	       ((string? right) (merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are empty, we're done merging.
	       ;; Return the result.
	       ((and (null? left)
		     (null? right))
		(reverse result))

	       ;; If LEFT and RIGHT still have elements to be
	       ;; processed, call PRED and run them through MERGE-AUX
	       ;; again.
	       ((and (not (null? left))
		     (not (null? right)))
		(if (pred (car left)
			  (car right))
		    (merge-aux pred
			       (cdr left)
			       right
			       (cons (car left) result))
		  (merge-aux pred
			     left
			     (cdr right)
			     (cons (car right) result))))

	       ;; If the cases above haven't matched, and LEFT is not
	       ;; NULL?, I call myself.
	       ((not (null? left))
		(merge-aux pred (cdr left) right (cons (car left) result)))

	       ;; Same as the previous case -- this time with RIGHT.
	       ((not (null? right))
		(merge-aux pred left (cdr right) (cons (car right) result)))

	       ;; We should never get here.
	       (else #f)))))
    (merge-aux pred l r '())))
{code}

Let's go through the COND clause-by-clause.  First, as our recursive base case,

h4. Recursive Merge Sort

h4. Bottom-up Merge Sort

We've already done the hard part by writing {{MERGE}}.  Now we just need to write the "driver" procedure that will call it.  Hence, =MERGE-SORT}}.  Take a few minutes to study the commented version:

{code}
(define (merge-sort xs pred)
  (let loop ((xs xs)
	     (result '()))
       (cond ((and (null? xs)
		   (null? (cdr result)))
	      (car result))
	     ((null? xs)
	      (loop result xs))
	     ((null? (cdr xs))
	      (loop (cdr xs)
		    (cons (car xs) result)))
	     (else
	      (loop (cddr xs)
		    (cons (merge <
				 (first xs)
				 (second xs))
			  result))))))
{code}

h3. Searching

Now that we've sorted a list of elements, we can search it.  It turns out that searching through a list of things is much faster if you can sort that list first.

In this section we'll look at a particular type of search algorithm called /binary search/.  Binary search is so named because it cuts the search space in half with every iteration.

Unlike some other searches, binary search only works on ordered lists of things.  That is why we had to go through the trouble of sorting our list earlier: so that we could search through it now.

h4. Binary search in words

Binary search works like this:

1. Pick the element in the middle of the list.
2. Is it the word you're looking for?

If yes, you're done.

If no, check it against the element you're looking for:

If it's /less than/ the element you're looking for:

Split the list in half at the current element

Search again, this time using only the "high half" of the list as input

If it's /greater than/ the element you're looking for:

split the list in half at the current element

search again, this time using only the "low half" of the list as input

h4. Binary search in pictures

INSERT IMAGE (GIF?) OF BINARY SEARCH ALGO HERE.

h4. Properties of Binary Search

h4. When to use binary search

h2. Stacks

h2. Trees

Binary trees.

CSRMs: constructors, selectors, recognizers, and mutators.

Basic operations

* creation
* insertion
* updating (destructive/in-place)
* deletion

Walking the tree using higher order functions (see notes from ADuni lectures).

Sorting with treesort.

This is only fast if the tree is balanced, so give the ``slow version'' first, since balanced trees are not introduced yet. Explain why it can be slow.

Balanced binary trees.

Red-black tree or AVL tree? AVL is supposedly simpler to implement but red-black is said to have superior tree rotation runtime -- once we have a self-balancing tree of either type we can write the ``fast'' treesort!

h3. Balancing Trees

h3. Searching Trees

h3. Using Trees to Implement Hash Tables

h2. Graphs

Introduction to graphs.

How to represent graphs with another data structure -- matrix, hash table, or association list. We might want to implement our own hash tables first using balanced binary trees -- this would be way cool!

In other words, it might be cool to build everything from the bottom up, e.g.:

1. Balanced binary tree
2. Hash Table
3. Graph (using hash table representation)

Discussion of common graph algorithms.

Traversal: TBD.

Search: Dijkstra's Algorithm.

Maybe include a discussion of {{longest-path.scm}} (implementation is currently incomplete)

h2. Heaps

h2. Glossary

[1] An iterative process is one that does not consume growing amounts of stack space while it runs.

{htmlcomment}
vim: set ft=confluencewiki:
Local Variables:
mode: confluence-markup
outline-regexp: "h[123456]. "
End:
{htmlcomment}
