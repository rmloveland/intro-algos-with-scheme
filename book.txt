* An Invitation to Algorithms with Scheme

by [Richard M. Loveland|mailto:r@rmloveland.com]

 {toc:minlevel=2|maxlevel=2}

** Preface

**** Why another book about algorithms?

There are many books about algorithms.  Most provide detailed analyses of topics such as "big O notation" and are aimed at an academic audience.  As a result, they strive to be comprehensive, and in so doing, run to hundreds of pages in length.

This book is different, because its goals are different.  First, it is an _invitation_ to algorithms; this means that we will cover one or two common algorithms in each of the following areas:

+ Sorting
+ Searching
+ Trees
+ Graphs

You can get surprisingly far with these simple building blocks!

Further, it is an invitation to algorithms _with Scheme_.  This means that we will use Scheme to describe the algorithms we want the computer to perform.  Along the way, we will solve real problems: disk I/O, parsing input file formats, and so on.  This will take us out of the realm of "pure" textbook algorithms and into the messy but fun world of real programming.  Because programming isn't much fun unless you can get the computer to actually do stuff.

Finally, it is my opinion that the notation provided by Scheme provides a clear description of many common algorithms.  It is even thought of by some programmers as beautiful in its own right, but I leave that for you to decide.

**** Prerequisites

 *Familiarity with Scheme:*

This book does not try to teach the Scheme language.  We assume you have already encountered an introduction to Scheme elsewhere. In particular, you should be pretty comfortable with recursion, since we'll be using plenty of it.

A good book that serves this purpose is [The Little Schemer|https://www.amazon.com/Little-Schemer-Daniel-P-Friedman/dp/0262560992/ref=sr_1_1?ie=UTF8&qid=1489850762&sr=8-1&keywords=the+little+schemer], by Dan Friedman & Matthias Felleisen.

For a more advanced introduction to the Scheme language, see Dybvig's excellent [The Scheme Programming Language|http://scheme.com/tspl4/].

 *The Scheme Interpreter:*

This book uses {{scsh}} 0.6.7.  {{Scsh}} is a Scheme interpreter designed to integrate closely with UNIX-like systems.  It also happens to be a pretty good Scheme implementation.

For more information about {{scsh}}, see [scsh.net|http://www.scsh.net].

You'll need access to a computer (or virtual machine) running Mac OS X or Linux to run the interpreter.  You should also be able to run it on Windows, using the [Windows Subsystem for Linux|https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux].

For instructions on installing {{scsh}} on each of these platforms, see [Appendix A|#appendix-a-building-scsh].

**** Conventions

Procedure names are written like this: {{CHAR-READY?}}

Code samples are usually set off from the surrounding text and look like this:

{code}
(define (double n)
  (+ n n))
{code}

** Introduction

What is an algorithm?

The three (or so) main ideas in this book:

1. Divide and conquer
2. Recursion
3. ???

Overview of the chapters.

Testing library.

Utilities library.

Discussion of type annotations and return values in procedure definitions. Facilitates skim-reading code.

Building a tags file for source navigation.

** Sorting

Although ostensibly, computers gonna compute, it turns out that most of the time we use them for their data processing capabilities more than as pure calculators.  A lot of what we do with computers (some would argue almost all of it) has to do with shuffling data around from one place to the next (or from one format into the next).  As a result, computers are beginning to become external brains that we dump things into for later use.

As external brains, one of the most important things computers help us with is in finding things.  As it happens, it's much easier to search for and find things if you put them in order first.  This is as true for computers as it is for your house.

That's why we'll start with sorting.

There are a number of different sorting algorithms with different properties.  In this book we'll focus on [Merge Sort|https://en.wikipedia.org/wiki/Merge_sort].  How come? It has a few qualities that make it worth using: 

1. It performs reasonably well, and provides a good tradeoff between implementation complexity and overall performance.
2. It is a good example of the general "divide and conquer" strategy for designing algorithms.
3. It suits Scheme due to the way lists (cons cells, really) are implemented.

In this chapter we will look at several implementations of merge sort, starting with the naive tree-recursive version you can easily find on the internet, and finishing up with a relatively performant iterative implementation.

If you can understand the implementation of merge sort described in this chapter, you should have no problem learning more about (and implementing your own versions of) quicksort, heap sort, and the like that you come across in other books or on the web.

***** Architecture of a Merge Sort Implementation

Merge sort is usually implemented with an architecture that consists of two parts: {{MERGE}}, a lower-level "helper" procedure that merges two partially sorted lists, and {{MERGE-SORT}}, a top-level procedure that internally calls {{MERGE}} repeatedly to get the list sorted.

Let's look at each of these procedures in turn.

{{MERGE}} handles splicing two sublists together in an ordering determined by a predicate such as {{<}}.  In other words, it puts things in order based on which of them are "less than" the other (this can have different meanings for numbers and strings).  Here are some examples showing the expected output.

{code}
(merge < '(2 191) '(18 45))
;; => (2 18 45 191)

(merge string<? '("scheme" "hacking") '("is" "fun"))
;; => ("is" "fun" "scheme" "hacking")
{code}

{{MERGE-SORT}} is a higher-level procedure that we can think of as a "driver" for {{MERGE}}.  It handles calling {{MERGE}} repeatedly and splicing the successively larger partially sorted lists returned by calls to {{MERGE}} together into one that is fully sorted.  Here are some examples showing its expected output:

{code}
(merge-sort '(17 51 55 13 12 75 98 48 98 89 68 86 89 51 73 18 92) <)
;; => (12 13 17 18 48 51 51 55 68 73 75 86 89 89 92 98 98)

(merge-sort '("i" "am" "a" "little" "teapot" "short" "and" "stout") string<?)
;; => FIXME: MERGE-AUX broken for strings, something is assuming numbers.
{code}

***** How MERGE Works

As we just saw, before we can do a merge sort, we need to write the =MERGE}} procedure.  Remember that it splices two lists together in an ordering determined by a predicate such as {{<}}.  (For expository purposes, we will stick to numbers for the rest of this implementation.)

An easy way to do this work is to walk two lists, /A/ and /B/, comparing the elements of each list in turn.  We will keep another list, {{RESULT}}, where we will put our, er, result.  If we assume =i}} is the current position in the list, and if {{(list-ref A i)}} (the current element from A) is less than {{(list-ref B i)}} (the current element from B), we push {{(list-ref A i)}} onto the results list.  Otherwise, we push {{(list-ref B i)}} onto the results list.

IMAGE

Finally, having traversed both of our input lists A and B, and processed all the elements of each, we return {{RESULT}}.

IMAGE

It's important to note that {{RESULT}} is *not* a sorted list.  It has been put into "pairwise order".  This is a fancy way of saying that we only compared two elements at a time, one each from /A/ and /B/, as we were building it.

At this point, you can probably begin to see why {{MERGE-SORT}} calls =MERGE}} over and over when it's sorting a list.

***** Implementing MERGE

Let's begin implementing MERGE by reminding ourselves of its specification.  MERGE takes as its arguments a predicate and two lists, and it returns a `pairwise ordered' list.  Another way of writing this is as follows (you may have noticed this notation above):

{code}
; MERGE : Pred List List -> List
{code}

This is just a way of annotating our program code that serves as a form of documentation.  It makes it a little easier to remember what a procedure does when you've been away from it for a while. Given this spec, we can start our implementation:

{code}
; MERGE : Pred List List -> List
(define (merge pred A B)
  (let ((return '()))
    return))
{code}

This meets the spec, in that it takes the right arguments and returns a list, but it doesn't actually do anything.  In fact, because we already know we're going to be returning a list, let's write it in a recursive style that uses an explicit list argument. This explicit list will be the value of RETURN we talked about earlier.  It also means we need to update our spec slightly.

{code}
; MERGE : Pred List List List -> List
(define (merge pred A B RETURN)
  RETURN)
{code}

Based on our above descriptions of the algorithm, the implementation of MERGE is straightforward, if a bit long:

{code}
(define (merge pred l r)
  (letrec ((merge-aux
	    (lambda (pred left right result)
	      (cond 
	       ;; If LEFT and RIGHT are both numbers, listify them so
	       ;; MERGE-AUX can work with them.
	       ((and (number? left)
		     (number? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If LEFT is just a number, listify it so MERGE-AUX
	       ;; can work with it.
	       ((number? left)
		(merge-aux pred (list left) right result))

	       ;; Likewise, if RIGHT is just a number, listify it for
	       ;; MERGE-AUX.
	       ((number? right)
		(merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are both strings, listify them so
	       ;; MERGE-AUX can use them.
	       ((and (string? left)
		     (string? right))
		(merge-aux pred (list left) (list right) result))

	       ;; If either LEFT or RIGHT are just strings, listify
	       ;; for MERGE-AUX.
	       ((string? left) (merge-aux pred (list left) right result))
	       ((string? right) (merge-aux pred left (list right) result))

	       ;; If LEFT and RIGHT are empty, we're done merging.
	       ;; Return the result.
	       ((and (null? left)
		     (null? right))
		(reverse result))

	       ;; If LEFT and RIGHT still have elements to be
	       ;; processed, call PRED and run them through MERGE-AUX
	       ;; again.
	       ((and (not (null? left))
		     (not (null? right)))
		(if (pred (car left)
			  (car right))
		    (merge-aux pred
			       (cdr left)
			       right
			       (cons (car left) result))
		  (merge-aux pred
			     left
			     (cdr right)
			     (cons (car right) result))))

	       ;; If the cases above haven't matched, and LEFT is not
	       ;; NULL?, I call myself.
	       ((not (null? left))
		(merge-aux pred (cdr left) right (cons (car left) result)))

	       ;; Same as the previous case -- this time with RIGHT.
	       ((not (null? right))
		(merge-aux pred left (cdr right) (cons (car right) result)))

	       ;; We should never get here.
	       (else #f)))))
    (merge-aux pred l r '())))
{code}

Let's go through the COND clause-by-clause.  First, as our recursive base case,

***** Recursive Merge Sort

***** Bottom-up Merge Sort

We've already done the hard part by writing {{MERGE}}.  Now we just need to write the "driver" procedure that will call it.  Hence, =MERGE-SORT}}.  Take a few minutes to study the commented version:

{code}
(define (merge-sort xs pred)
  (let loop ((xs xs)
	     (result '()))
       (cond ((and (null? xs)
		   (null? (cdr result)))
	      (car result))
	     ((null? xs)
	      (loop result xs))
	     ((null? (cdr xs))
	      (loop (cdr xs)
		    (cons (car xs) result)))
	     (else
	      (loop (cddr xs)
		    (cons (merge <
				 (first xs)
				 (second xs))
			  result))))))
{code}

** Searching

Now that we've sorted a list of elements, we can search it.  It turns out that searching through a list of things is much faster if you can sort that list first.

In this section we'll look at a particular type of search algorithm called /binary search/.  Binary search is so named because it cuts the search space in half with every iteration.

Unlike some other searches, binary search only works on ordered lists of things.  That is why we had to go through the trouble of sorting our list earlier: so that we could search through it now.

***** Binary search in words

Binary search works like this:

1. Pick the element in the middle of the list.
2. Is it the word you're looking for?

If yes, you're done.

If no, check it against the element you're looking for:

If it's _less than_ the element you're looking for:

Split the list in half at the current element

Search again, this time using only the "high half" of the list as input

If it's _greater than_ the element you're looking for:

split the list in half at the current element

search again, this time using only the "low half" of the list as input

***** Binary search in pictures

INSERT IMAGE (GIF?) OF BINARY SEARCH ALGO HERE.

***** Properties of Binary Search

***** When to use binary search

** Trees

*** Binary trees

CSRMs: constructors, selectors, recognizers, and mutators.

 *Basic operations*

+ creation
+ insertion
+ updating (destructive/in-place)
+ deletion

Walking the tree using higher order functions (see notes from ADuni lectures).

 *Sorting with treesort*

This is only fast if the tree is balanced, so give the "slow version" first, since balanced trees are not introduced yet. Explain why it can be slow.

 *Balanced binary trees*

Red-black tree or AVL tree? AVL is supposedly simpler to implement but red-black is said to have superior tree rotation runtime -- once we have a self-balancing tree of either type we can write the "fast" treesort!

**** Balancing Trees

**** Searching Trees

**** Practical: Using Trees to Implement Hash Tables

** Graphs

*** Overview of Graphs

How to represent graphs with another data structure: matrix, hash table, or association list. We might want to implement our own hash tables first using balanced binary trees -- that would be way cool!

In other words, it might be cool to build everything from the bottom up, e.g.:

1. Balanced binary tree
2. Hash Table
3. Graph (using hash table representation)

Discussion of common graph algorithms.

Traversal: TBD.

Search: Dijkstra's Algorithm.

Maybe include a discussion of {{longest-path.scm}} (implementation is currently incomplete)

*** Graph Traversal

Write up the {{longest-path.scm}} code here.

*** Graph Search

!winston-horn-network.png!

I've been having fun translating some of the code in Winston and Horn's _Lisp_ into Scheme.  This book is amazing -- clearly written, with lots of motivating examples and applications.  As SICP is to language implementation, _Lisp_ is to application development, with chapters covering constraint propagation, forward and backward chaining, simulation, object-oriented programming, and so on.  And it does include the obligatory Lisp interpreter in one chapter, if you're into that sort of thing.

In this installment, based on Chapter 19, we will look at some simple strategies for searching for a path between two nodes on a network (a graph).  The network we'll be using is shown in the diagram above.

Here's the same network, represented as an alist where each {{CAR:CDR}} pair represents a {{NODE:NEIGHBORS}} relationship:

{code}
  '((f e)
    (e b d f)
    (d s a e)
    (c b)
    (b a c e)
    (a s b d)
    (s a d))
{code}

The high-level strategy the authors use is to traverse the network, building up a list of partial paths.  If a partial path ever reaches the point where it describes a full path between the two network nodes we're after, we've been successful.

As with trees, we can do either a breadth-first or depth-first traversal.  Here's what the intermediate partial paths will look like for a breadth-first traversal that builds a path between nodes {{S}} and {{F}}:

{code}
  (s)
  (s a)
  (s d)
  (s a b)
  (s a d)
  (s d a)
  (s d e)
  (s a b c)
  (s a b e)
  (s a d e)
  (s d a b)
  (s d e b)
  '(s d e f)
{code}

Based on that output, we can deduce that every time we visit a node, we want to extend our partial paths list with that node.  Here's one option -- its only problem is that it will happily build circular paths that keep us from ever finding the node we want:

{code}
  (define (%buggy-extend path)             ;builds circular paths
    (map (lambda (new-node) 
           (cons new-node path))
         (%get-neighbor (first path))))
{code}

(Incidentally, I've become fond of the convention whereby internal procedures that aren't part of a public-facing API are prefixed with the `%' character.  This can be found in some parts of the MIT Scheme sources, and I believe it's used in Racket as well.  I've started writing lots of my procedures using this notation to remind me that the code I'm writing is not the real `API', that the design will need more work, and that the current code is just a first draft.  I'm using that convention here.)

Here's a better version that checks if we've already visited the node before adding it to the partial paths list -- as a debugging aid it prints out the current path before extending it:

{code}
  (define (%extend path)
    (display (reverse path))
    (newline)
    (map (lambda (new-node)
           (cons new-node path))
         (filter (lambda (neighbor)
                   (not (member neighbor path)))
                 (%get-neighbor (first path)))))
{code}

You may have noticed the {{%GET-NEIGHBOR}} procedure; it's just part of some silly data structure bookkeeping code.  Please feel free to deride me in the comments for my use of a global variable.  What can I say?  I'm Scheming like it's 1988 over here!  Here's the boilerplate:

{code}
  (define *neighbors* '())
  
  (define (%add-neighbor! k v)
    (let ((new-neighbor (cons k v)))
      (set! *neighbors*
            (cons new-neighbor *neighbors*))))
  
  (define (%get-neighbor k)
    (let ((val (assoc k *neighbors*)))
      (if val
          (cdr val)
          '())))
  
  (%add-neighbor! 's '(a d))
  (%add-neighbor! 'a '(s b d))
  (%add-neighbor! 'b '(a c e))
  (%add-neighbor! 'c '(b))
  (%add-neighbor! 'd '(s a e))
  (%add-neighbor! 'e '(b d f))
  (%add-neighbor! 'f '(e))
{code}

Now that we have our data structure and a way to extend our partial path list (non-circularly), we can write the main search procedure, {{%BREADTH-FIRST}}.  The authors have a lovely way of explaining its operation:

{quote}
BREADTH-FIRST is said to do a "breadth-first" search because it extends all partial paths out to uniform length before extending any to a greater length.
{quote}

Here's the code, translated to use a more Schemely, iterative named {{LET}} instead of the linear-recursive definition from the book:

{code}
  (define (%breadth-first start finish network)
    (let ((queue (list (list start))))
      (let loop ((start start)
                 (finish finish)
                 (network network)
                 (queue queue))
        (cond ((null? queue) '())                    ;Queue empty?
              ((equal? finish (first (first queue))) ;Finish found?
               (reverse (first queue)))              ;Return path.
              (else
               (loop start
                     finish               ;Try again.
                     network
                     (append
                      (rest queue)
                      (extend (first queue))))))))) ;New paths in front.
{code}

(A better way to write this procedure would be to implement a generic internal search procedure that takes its `breadthiness' or `depthiness' as a parameter.  We could then wrap it with nicer public-facing search procedures specific names.)

Meanwhile, back at the REPL... we remind ourselves of what {{*NEIGHBORS*}} actually looks like, and then we search for a path between the nodes {{S}} and {{F}}.

{code}
> *neighbors*
'((f e) (e b d f) (d s a e) (c b) (b a c e) (a s b d) (s a d))
> (%breadth-first 's 'f *neighbors*)
(s)
(s a)
(s d)
(s a b)
(s a d)
(s d a)
(s d e)
(s a b c)
(s a b e)
(s a d e)
(s d a b)
(s d e b)
'(s d e f)
{code}

What fun!  I can almost imagine using a three-dimensional variant of these searches for a space wargame with wormhole travel!  Except, you know, they'd need to be much faster and more skillfully implemented.  There's also the tiny requirement to write the surrounding game...

It shouldn't need to be said, but: Of course the authors knew better; they were trying to hide that unnecessary complexity from you until later.

** Real World: Hash Tables

In this chapter, we're going to implement our own hash tables.

In day-to-day programming, the hash table is probably the most important real-world data structure.

The hash table also gives us a nice real-world proving ground for our algorithms skills, since implementing hash tables requires putting together several different data structures into one - in other words, it is a _compound data structure_.

** Glossary

*** Iterative process

<a name="glossary-iterative-process" />

An iterative process is one that does not consume growing amounts of stack space while it runs.  In terms of Scheme code, code that describes an iterative process usually looks like this:

{code}
(define (+ a b)
  (if (= a 0)
      b
      (+ (decr a)
         (incr b))))
{code}

You can visualize the operation of an iterative like this:

{code}
(+ 4 3)
(+ 3 4)
(+ 2 5)
(+ 1 6)
(+ 0 7)
;; => 7
{code}

Notice how the "shape" of the successive calls to {{+}} stay the same "size"?

Using [Big O notation|#glossary-big-o-notation], you can say that {{+}} uses _O(1)_ space (memory), and _O(n)_ time (CPU).

*** Recursive Process

<a name="glossary-recursive-process" />

A recursive process is one that consumes growing amounts of stack space while it runs.  It terms of Scheme code, code that describes a recursive process usually looks like this:

{code}
(define (incr n) (+ n 1))
(define (decr n) (- n 1))

(define (+ a b)
  (if (= a 0)
      b
      (incr (+ (decr a) b))))
{code}

You can visualize the operation of a recursive process like this:

{code}
(+ 3 4)
(incr (+ (decr 3) 4))
(incr (incr (+ (decr 2) 4)))
(incr (incr (incr (+ (decr 1) 4))))
(incr (incr (incr 4)))
7
{code}

Notice how the "shape" of the successive calls to {{+}} get "larger"?

Using [Big O notation|#glossary-big-o-notation], you can say that {{+}} uses _O(n)_ time in its first argument and _O(n)_ space in its second.

*** Big O Notation

<a name="glossary-big-o-notation" />

"Big O" notation is a way to talk about the resource usage of an algorithm.  This usage can be along several axes:

+ Time (CPU - how many instructions will it take to compute?)
+ Space (Memory - how much storage will it use?)

Instead of "resource usage" you can also say the "complexity" of the algorithm.  This is the term you are likely to find in more academic writings.

To be more precise, this notation describes the "upper bound" of the resource usage.  This means that, even in the worst case scenario, the algorithm will not use more than a given amount of resources.

For more details, check out the following references:

+ [A beginner's guide to Big O notation|https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/]: Nice short article to get your feet wet with.

+ [What is a plain English explanation of "Big O notation?|http://stackoverflow.com/questions/487258/what-is-a-plain-english-explanation-of-big-o-notation] - the first answer provides a really good overview.  If you don't understand some of the mathier parts, just skip over them.

+ [Big-O Cheat Sheet|http://bigocheatsheet.com/]: This page has a nice big graph that makes it easy to visualize the different complexities.  Further down the page there are tables that list algorithm complexities for various operations (insert, delete, search) on data structures such as stacks, lists, hash tables, etc.  Add this to your bookmarks so you can refer back to it as needed.

** Bibliography

+ Abelson & Sussman, _Structure and Interpretation of Computer Programs_, 1st ed.

+ Winston & Horn, _Lisp_, X ed.

** Appendix

<a name="appendix-a-building-scsh" />

*** A. Building scsh

Describe the process of building scsh here.
